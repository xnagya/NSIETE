{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Neural network models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torchvision import models\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_regular(nn.Module):\n",
    "    def __init__(self, embedding_dims, hidden_size, drop = 0):\n",
    "        super().__init__()\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "\n",
    "        # Network parameters\n",
    "        self.W = nn.Parameter(torch.Tensor(embedding_dims, hidden_size * 4))\n",
    "        self.U = nn.Parameter(torch.Tensor(hidden_size, hidden_size * 4))\n",
    "        self.bias = nn.Parameter(torch.Tensor(4 * hidden_size))\n",
    "\n",
    "        # Add dropout layer\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.init_weights(hidden_size)\n",
    "\n",
    "\n",
    "    def init_weights(self, hidden_size):\n",
    "        stdv = 1.0 / math.sqrt(hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        seq_length = input.shape(1)\n",
    "        output = []\n",
    "\n",
    "        # Forward pass for each word \n",
    "        for i in range(seq_length):\n",
    "            x = input[:,i,:]\n",
    "\n",
    "            x, state = self.forward_cell(x, state)\n",
    "\n",
    "            # Save results \n",
    "            output.append(x)\n",
    "        \n",
    "        # Join results and reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        output = torch.cat(output, dim=0)\n",
    "        \n",
    "        output = output.transpose(0, 1).contiguous()\n",
    "\n",
    "        return output, state\n",
    "        \n",
    "\n",
    "    # Computes forward for one timestep (one word of sequence)\n",
    "    def forward_cell(self, x_t, cell_states):\n",
    "        # load current state of cell\n",
    "        h_t, c_t = cell_states\n",
    "\n",
    "        # Squeeze dims if they equal 1 \n",
    "        h_t = h_t.squeeze(dim=0)\n",
    "        c_t = c_t.squeeze(dim=0)\n",
    "\n",
    "        # Forward pass\n",
    "        gates = torch.matmul(x_t, self.W) + torch.matmul(h_t, self.U) + self.bias\n",
    "\n",
    "        gates = gates.squeeze()\n",
    "\n",
    "        # Devide vector into gates\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "\n",
    "        # Compute state of each gate\n",
    "        ingate = F.sigmoid(ingate)\n",
    "        forgetgate = F.sigmoid(forgetgate)\n",
    "        cellgate = F.tanh(cellgate)\n",
    "        outgate = F.sigmoid(outgate)\n",
    "        \n",
    "        # Compute new cell state\n",
    "        c_t = torch.mul(forgetgate, c_t) +  torch.mul(ingate, cellgate)        \n",
    "        h_t = torch.mul(outgate, F.tanh(c_t))\n",
    "        \n",
    "        c_t = c_t.unsqueeze(0)\n",
    "        h_t = h_t.unsqueeze(0)\n",
    "        \n",
    "        return self.dropout(h_t), (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, rnn_type, output_size, vocab_size, embed_dims, params: dict):\n",
    "        super().__init__()\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        \n",
    "        self.rnn_type = rnn_type\n",
    "        assert self.rnn_type in ['test', 'lstm', 'lstm_M', 'lstm_A'], f\"RNN type '{self.rnn_type} 'is NOT supported.\"\n",
    "\n",
    "        # Config parameters\n",
    "        try:\n",
    "            drop = params['embedding_dropout']\n",
    "            layer_count = params['lstm_layers']\n",
    "            dir2 = params['bidirectional']\n",
    "            pad_idx = params['padding_index']\n",
    "            lstm_hidden = params['lstm_features']\n",
    "            drop_lstm = params['lstm_dropout'] \n",
    "        except KeyError as e:\n",
    "            raise Exception(f'Parameter \"{e.args[0]}\" NOT found!')\n",
    "        \n",
    "        # Encoder layer = encodes indices of words to embedding vectors\n",
    "        self.encoder = nn.Embedding(\n",
    "            num_embeddings = vocab_size\n",
    "            , embedding_dim = embed_dims\n",
    "            , padding_idx = pad_idx\n",
    "            )\n",
    "\n",
    "        # Dropout layer = drops embedding features\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "\n",
    "        # Initlize LSTM layers\n",
    "        self.rnns = nn.ModuleList()\n",
    "\n",
    "        match self.rnn_type:\n",
    "            # LSTM - pytorch\n",
    "            case \"test\":\n",
    "                l = nn.LSTM(\n",
    "                    input_size = embed_dims,\n",
    "                    hidden_size = lstm_hidden, \n",
    "                    num_layers=layer_count, \n",
    "                    bidirectional=dir2, \n",
    "                    dropout=drop_lstm, \n",
    "                    batch_first= True \n",
    "                )\n",
    "\n",
    "                for name, w in l.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        nn.init.normal_(w)\n",
    "\n",
    "                self.rnns.append(l)\n",
    "\n",
    "\n",
    "            # LSTM - regular\n",
    "            case \"lstm\":\n",
    "                for i in range(lstm_hidden):\n",
    "                    if (i == lstm_hidden - 1):\n",
    "                        self.rnns.append(LSTM_regular(embed_dims, lstm_hidden))\n",
    "                    else:\n",
    "                        self.rnns.append(LSTM_regular(embed_dims, lstm_hidden, drop_lstm))\n",
    "                    \n",
    "                \n",
    "            # LSTM - momentum\n",
    "            case \"lstm_M\":\n",
    "                pass\n",
    "            \n",
    "            # LSTM - momentum ADAM\n",
    "            case \"lstm_A\":\n",
    "                pass\n",
    "        \n",
    "        # Decoder layer = output layer for network\n",
    "        self.decoder = nn.Linear(lstm_hidden, output_size)\n",
    "\n",
    "        # Initialize lstm state\n",
    "        mul = 2 if dir2 else 1\n",
    "        self.state = (torch.zeros(layer_count * mul, lstm_hidden),torch.zeros(layer_count * mul, lstm_hidden))\n",
    "\n",
    "    def forward(self, train: bool, input):\n",
    "        # Embeddings\n",
    "        input = self.encoder(input)\n",
    "\n",
    "        # Dropout \n",
    "        if train:\n",
    "            input = self.dropout(input)\n",
    "\n",
    "        # Initialize first state\n",
    "        if lstm_state is None:\n",
    "            hx = torch.zeros_like(input)\n",
    "            cx = torch.zeros_like(input)\n",
    "            lstm_state = (hx, cx)\n",
    "\n",
    "       # Compute LSTM forward for each layer\n",
    "        for lstm in self.rnns:\n",
    "            input, lstm_state = lstm.forward(input, lstm_state)\n",
    "\n",
    "        input = self.decoder(input.view(input.size(0)*input.size(1), input.size(2)))\n",
    "        return input.view(input.size(0), input.size(1), input.size(1)), lstm_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10, 5])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_input\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     14\u001b[0m net \u001b[38;5;241m=\u001b[39m RNN(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_size, vocab_size, embed_features, config_to_dict(config_NN))\n\u001b[1;32m---> 15\u001b[0m output \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28;01mFalse\u001b[39;00m, test_input)\n",
      "Cell \u001b[1;32mIn[24], line 78\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, train, input)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, train: \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# Embeddings\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# Dropout \u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train:\n",
      "File \u001b[1;32mc:\\Users\\matul\\anaconda3\\envs\\NN\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\matul\\anaconda3\\envs\\NN\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matul\\anaconda3\\envs\\NN\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[1;32mc:\\Users\\matul\\anaconda3\\envs\\NN\\Lib\\site-packages\\torch\\nn\\functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "from net_config import *\n",
    "\n",
    "vocab_size = 100\n",
    "output_size = 10\n",
    "\n",
    "# Input tensor\n",
    "batch_size = 3\n",
    "seq_length = 10\n",
    "embed_features = 5\n",
    "\n",
    "test_input = torch.LongTensor(batch_size, seq_length, embed_features).random_()\n",
    "\n",
    "print(test_input.shape)\n",
    "\n",
    "net = RNN(\"test\", output_size, vocab_size, embed_features, config_to_dict(config_NN))\n",
    "output = net.forward(False, test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "tensor([[-0.5228, -3.3039, -1.7445]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.randn(1, 2)\n",
    "B = torch.randn(2, 3)\n",
    "\n",
    "# works\n",
    "C = torch.matmul(A, B)\n",
    "print(C.shape)\n",
    "print(C)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
