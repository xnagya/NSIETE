{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Neural network models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torchvision import models\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_regular(nn.Module):\n",
    "    def __init__(self, embedding_dims, hidden_size, drop = 0):\n",
    "        super().__init__()\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "\n",
    "        # Network parameters\n",
    "        self.W = nn.Parameter(torch.Tensor(embedding_dims, hidden_size * 4))\n",
    "        self.U = nn.Parameter(torch.Tensor(hidden_size, hidden_size * 4))\n",
    "        self.bias = nn.Parameter(torch.Tensor(4 * hidden_size))\n",
    "\n",
    "        # Add dropout layer\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.init_weights(hidden_size)\n",
    "\n",
    "\n",
    "    def init_weights(self, hidden_size):\n",
    "        stdv = 1.0 / math.sqrt(hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        seq_length = input.shape(1)\n",
    "        output = []\n",
    "\n",
    "        # Forward pass for each word \n",
    "        for i in range(seq_length):\n",
    "            x = input[:,i,:]\n",
    "\n",
    "            x, state = self.forward_cell(x, state)\n",
    "\n",
    "            # Save results \n",
    "            output.append(x)\n",
    "        \n",
    "        # Join results and reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        output = torch.cat(output, dim=0)\n",
    "        # TODO check\n",
    "        output = output.transpose(0, 1).contiguous()\n",
    "\n",
    "        return output, state\n",
    "        \n",
    "\n",
    "    # Computes forward for one timestep (one word of sequence)\n",
    "    def forward_cell(self, x_t, cell_states):\n",
    "        # load current state of cell\n",
    "        h_t, c_t = cell_states\n",
    "\n",
    "        # Squeeze dims if they equal 1 \n",
    "        h_t = h_t.squeeze(dim=0)\n",
    "        c_t = c_t.squeeze(dim=0)\n",
    "\n",
    "        # Forward pass\n",
    "        gates = torch.matmul(x_t, self.W) + torch.matmul(h_t, self.U) + self.bias\n",
    "\n",
    "        gates = gates.squeeze()\n",
    "\n",
    "        # Devide vector into gates\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "\n",
    "        # Compute state of each gate\n",
    "        ingate = F.sigmoid(ingate)\n",
    "        forgetgate = F.sigmoid(forgetgate)\n",
    "        cellgate = F.tanh(cellgate)\n",
    "        outgate = F.sigmoid(outgate)\n",
    "        \n",
    "        # Compute new cell state\n",
    "        c_t = torch.mul(forgetgate, c_t) +  torch.mul(ingate, cellgate)        \n",
    "        h_t = torch.mul(outgate, F.tanh(c_t))\n",
    "        \n",
    "        c_t = c_t.unsqueeze(0)\n",
    "        h_t = h_t.unsqueeze(0)\n",
    "        \n",
    "        return self.dropout(h_t), (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, rnn_type, output_size, vocab_size, embed_dims, params: dict):\n",
    "        super().__init__()\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        \n",
    "        self.rnn_type = rnn_type\n",
    "        assert self.rnn_type in ['test', 'lstm', 'lstm_M', 'lstm_A'], f\"RNN type '{self.rnn_type} 'is NOT supported.\"\n",
    "\n",
    "        # Config parameters\n",
    "        try:\n",
    "            drop = params['embedding_dropout']\n",
    "            layer_count = params['lstm_layers']\n",
    "            dir2 = params['bidirectional']\n",
    "            pad_idx = params['padding_index']\n",
    "            lstm_hidden = params['lstm_features']\n",
    "            drop_lstm = params['lstm_dropout'] \n",
    "        except KeyError as e:\n",
    "            raise Exception(f'Parameter \"{e.args[0]}\" NOT found!')\n",
    "        \n",
    "        # Encoder layer = encodes indices of words to embedding vectors\n",
    "        self.encoder = nn.Embedding(\n",
    "            num_embeddings = vocab_size\n",
    "            , embedding_dim = embed_dims\n",
    "            , padding_idx = pad_idx\n",
    "            )\n",
    "\n",
    "        # Dropout layer = drops embedding features\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "\n",
    "        # Initlize LSTM layers\n",
    "        self.rnns = nn.ModuleList()\n",
    "\n",
    "        match self.rnn_type:\n",
    "            # LSTM - pytorch\n",
    "            case \"test\":\n",
    "                l = nn.LSTM(\n",
    "                    input_size = embed_dims,\n",
    "                    hidden_size = lstm_hidden, \n",
    "                    num_layers=layer_count, \n",
    "                    bidirectional=dir2, \n",
    "                    dropout=drop_lstm, \n",
    "                    batch_first= True \n",
    "                )\n",
    "                for p in l.parameters():\n",
    "                    # TODO init weights\n",
    "                    pass\n",
    "\n",
    "                self.rnns.append(l)\n",
    "\n",
    "            # LSTM - regular\n",
    "            case \"lstm\":\n",
    "                for i in range(lstm_hidden):\n",
    "                    if (i == lstm_hidden - 1):\n",
    "                        self.rnns.append(LSTM_regular(embed_dims, lstm_hidden))\n",
    "                    else:\n",
    "                        self.rnns.append(LSTM_regular(embed_dims, lstm_hidden, drop_lstm))\n",
    "                    \n",
    "                \n",
    "            # LSTM - momentum\n",
    "            case \"lstm_M\":\n",
    "                pass\n",
    "            \n",
    "            # LSTM - momentum ADAM\n",
    "            case \"lstm_A\":\n",
    "                pass\n",
    "        \n",
    "        # Decoder layer = output layer for network\n",
    "        self.decoder = nn.Linear(lstm_hidden, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, train: bool, input, lstm_state = None):\n",
    "        # Embeddings\n",
    "        input = self.encoder(input)\n",
    "\n",
    "        # Dropout \n",
    "        if train:\n",
    "            input = self.dropout(input)\n",
    "\n",
    "        # Initialize first state\n",
    "        if lstm_state is None:\n",
    "            hx = 0\n",
    "            cx = 0\n",
    "            lstm_state = (hx, cx)\n",
    "\n",
    "       # Compute LSTM forward for each layer\n",
    "        for lstm in self.rnns:\n",
    "            input, lstm_state = lstm.forward(input, lstm_state)\n",
    "\n",
    "        # TODO check\n",
    "        input = self.decoder(input.view(input.size(0)*input.size(1), input.size(2)))\n",
    "        return input.view(input.size(0), input.size(1), input.size(1)), lstm_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10, 5])\n"
     ]
    }
   ],
   "source": [
    "from net_config import *\n",
    "\n",
    "vocab_size = 100\n",
    "output_size = 10\n",
    "\n",
    "# Input tensor\n",
    "batch_size = 3\n",
    "seq_length = 10\n",
    "embed_features = 5\n",
    "\n",
    "test_input = torch.randn(batch_size, seq_length, embed_features)\n",
    "print(test_input.shape)\n",
    "\n",
    "net = RNN(\"test\", output_size, vocab_size, embed_features, config_to_dict(config_NN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "tensor([[-0.1391, -1.2906,  1.2185]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.randn(1, 2)\n",
    "B = torch.randn(2, 3)\n",
    "\n",
    "# works\n",
    "C = torch.matmul(A, B)\n",
    "print(C.shape)\n",
    "print(C)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
