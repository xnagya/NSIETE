{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Model trainer__\n",
    "This file contains Trainer and Statistics classes used during training of NN models. All metrics are calculated using library _torchmetrics_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score, MulticlassAUROC\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import net_config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EssayDataset(Dataset):\n",
    "    def __init__(self, file_essay, file_missing):\n",
    "        essay_arr = np.load(file_essay)\n",
    "        indexes_arr = np.load(file_missing, allow_pickle=True)\n",
    "\n",
    "        assert (essay_arr.shape[0] == indexes_arr.shape[0]), f\"Wrong dataset size, essey count = {essay_arr.shape[0]} indexes count = {indexes_arr.shape[0]}\"\n",
    "        essey_count = essay_arr.shape[0]\n",
    "\n",
    "        self.inputs = torch.from_numpy(essay_arr).type(torch.long)\n",
    "\n",
    "        # List of tensors with target words\n",
    "        self.targets = []\n",
    "\n",
    "        # List of tensors with missing words index\n",
    "        self.missing_positions = []\n",
    "\n",
    "        # Extract missing words and their positions from numpy\n",
    "        for i in range(essey_count):\n",
    "            missing_words = []\n",
    "            missing_pos = []\n",
    "\n",
    "            for pair in indexes_arr[i]:\n",
    "                pos = pair[0]\n",
    "                word_idx = pair[1]\n",
    "\n",
    "                missing_pos.append(pos)\n",
    "                missing_words.append(word_idx)\n",
    "\n",
    "            self.missing_positions.append(torch.tensor(missing_pos, dtype=torch.long))\n",
    "            self.targets.append(torch.tensor(missing_words, dtype=torch.long))\n",
    "\n",
    "            # Set missing word count per essay\n",
    "            if (i == 0): \n",
    "                self.missing_per_essay = len(missing_words)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.inputs[idx]\n",
    "        pos = self.missing_positions[idx]\n",
    "        y = self.targets[idx]\n",
    "\n",
    "        return x, pos, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Statistics:\n",
    "    def __init__(self):\n",
    "        self.metrics = dict()\n",
    "\n",
    "    def update(self, metric_name, new_value):\n",
    "        if metric_name in self.metrics:\n",
    "            values = self.metrics[metric_name]\n",
    "            values.append(new_value)\n",
    "        else:\n",
    "            values = [new_value]\n",
    "            self.metrics.update({metric_name : values})\n",
    "\n",
    "    def get_metric(self, metric_name):\n",
    "        return self.metrics.get(metric_name)\n",
    "    \n",
    "    def batch_count(self):\n",
    "        max = 0\n",
    "        for val in self.metrics.values():\n",
    "            if len(val) > max:\n",
    "                max = len(val)\n",
    "\n",
    "        return max \n",
    "    \n",
    "    def clear(self):\n",
    "        self.metrics = dict()\n",
    "\n",
    "    # First batch is 0\n",
    "    def batch_metrics(self, batch_num):\n",
    "        result = dict()\n",
    "        \n",
    "        for metric_name, values in self.metrics.items():\n",
    "            if (batch_num >= 0) and (batch_num < len(values)):\n",
    "                metric_val = values[batch_num]\n",
    "                result.update({metric_name : metric_val})\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def metric_average(self, metric_name):\n",
    "        if metric_name in self.metrics:\n",
    "            values = self.metrics[metric_name]\n",
    "            return float(sum(values) / len(values))\n",
    "        \n",
    "        else: \n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module, vocab_size):\n",
    "        # Select GPU device\n",
    "        self.device = (\n",
    "            \"cuda\" if torch.cuda.is_available()\n",
    "            else \"mps\" if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "\n",
    "        print(f\"Using {self.device} device for training\")\n",
    "\n",
    "        # Move model to available device\n",
    "        self.network = model.to(self.device)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.network.parameters()\n",
    "            , lr = cfg.learning_rate\n",
    "            , betas = cfg.betas\n",
    "            , weight_decay = cfg.weight_decay\n",
    "        )\n",
    "\n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "        # Class for saving metrics\n",
    "        self.stats = Statistics()\n",
    "\n",
    "        # Metrics \n",
    "        self.acc = MulticlassAccuracy(\n",
    "            num_classes = vocab_size\n",
    "            , average = \"macro\"\n",
    "            ).to(self.device)\n",
    "        self.roc = MulticlassAUROC(\n",
    "            num_classes = vocab_size\n",
    "            , average = \"macro\"\n",
    "            ).to(self.device)\n",
    "        self.f1 = MulticlassF1Score(\n",
    "            num_classes = vocab_size\n",
    "            , average = \"macro\"\n",
    "            ).to(self.device)\n",
    "\n",
    "        # Saving and loading model\n",
    "        self.best_model = None\n",
    "        self.best_accuracy = None\n",
    "\n",
    "    def load_dataset(self, essay_path, positions_path):\n",
    "        # Load dataset\n",
    "        dataset = EssayDataset(essay_path, positions_path)\n",
    "        \n",
    "        # Split dataset to train, validation and test \n",
    "        gen = torch.Generator().manual_seed(42)\n",
    "        data_train, data_val, data_test = random_split(dataset, [0.7, 0.15, 0.15], generator=gen)\n",
    "\n",
    "        # Create dataset loaders\n",
    "        self.data_train = DataLoader(data_train, batch_size = cfg.batch_size, shuffle = True)\n",
    "        self.data_val = DataLoader(data_val, batch_size = cfg.batch_size, shuffle = True)\n",
    "        self.data_test = DataLoader(data_test, batch_size = cfg.batch_size, shuffle = False)\n",
    "\n",
    "    def save_model(self, current_epoch):  \n",
    "        if self.best_model is not None:\n",
    "            checkpoint = {\n",
    "                'epoch': current_epoch,\n",
    "                'NNmodel': self.best_model.state_dict(),\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(checkpoint, cfg.model_path)\n",
    "            print(f\"NN model saved at path '{cfg.model_path}'\")\n",
    "\n",
    "    def load_model(self):\n",
    "        checkpoint = torch.load(cfg.model_path)\n",
    "\n",
    "        self.network.load_state_dict(checkpoint['NNmodel'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "        self.test_model()\n",
    "\n",
    "        print(f\"NN model loaded from path '{cfg.model_path}'\")\n",
    "        return checkpoint['epoch']\n",
    "\n",
    "    def train_model(self):\n",
    "        # Train model (dataset = data_train)\n",
    "        self.network.train()\n",
    "        start = time.time()\n",
    "\n",
    "        for x, pos, y in self.data_train:\n",
    "            x, pos, y = x.to(self.device), pos.to(self.device), y.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            x = self.network(x, pos, self.device)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = self.loss_fn(x.view(-1, x.shape[-1]), y.view(-1))\n",
    "\n",
    "            # Save batch loss\n",
    "            self.stats.update(\"loss_train\", loss)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(self.network.parameters(), cfg.grad_clip)\n",
    "            self.stats.update(\"grad\", grad_norm)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "        end = time.time()\n",
    "        print(f\"Train time in sec = {end - start}\")\n",
    "\n",
    "        # Evaulate model by calculating loss (dataset = data_val)\n",
    "        self.network.eval()\n",
    "        start = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, pos, y in self.data_val:\n",
    "                x, pos, y = x.to(self.device), pos.to(self.device), y.to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                x = self.network(x, pos, self.device)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = self.loss_fn(x.view(-1, x.shape[-1]), y.view(-1))\n",
    "\n",
    "                # Save batch loss\n",
    "                self.stats.update(\"loss_val\", loss)\n",
    "\n",
    "        end = time.time()\n",
    "        print(f\"Validation time in sec = {end - start}\")\n",
    "\n",
    "    # Test model by calculating metrics (dataset = data_test) and keep the best model\n",
    "    def test_model(self):\n",
    "        self.network.eval()\n",
    "        start = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, pos, y in self.data_test:\n",
    "                x, pos, y = x.to(self.device), pos.to(self.device), y.to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                x = self.network(x, pos, self.device)\n",
    "\n",
    "                classes = torch.argmax(x, dim=2)\n",
    "                confidence = torch.softmax(x, dim=2)\n",
    "                \n",
    "                y = y.view(-1)\n",
    "                classes = classes.view(-1)\n",
    "                confidence = confidence.view(-1, confidence.shape[2])\n",
    "\n",
    "                # Calculate metrics\n",
    "                accuracy = self.acc(classes, y).item()\n",
    "                self.stats.update(\"acc\", accuracy)\n",
    "                self.stats.update(\"f1\", self.f1(classes, y).item())\n",
    "                # Calculating auroc takes too long - about 8x increase for test time       \n",
    "                #self.stats.update(\"auroc\", self.roc(confidence, y).item())\n",
    "                \n",
    "        end = time.time()\n",
    "        print(f\"Test time in sec = {end - start}\")\n",
    "\n",
    "        # Save best model\n",
    "        if (self.best_accuracy is None) or (self.best_accuracy < accuracy):\n",
    "            self.best_accuracy = accuracy\n",
    "            self.best_model = self.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import models\n",
    "\n",
    "embedding_path = \"embedding_matrix.npy\"\n",
    "\n",
    "net = models.RNN(\"lstm\", embedding_path, cfg.config_to_dict(cfg.config_NN))\n",
    "t = Trainer(net, cfg.config_NN.vocab_size)\n",
    "\n",
    "path_essay = \"C:\\\\Users\\\\matul\\\\Desktop\\\\NSIETE\\\\zadanie3\\\\output\\\\essays_tensor_representation_max50_1miss.npy\"\n",
    "path_pos = \"C:\\\\Users\\\\matul\\\\Desktop\\\\NSIETE\\\\zadanie3\\\\output\\\\position_index_pairs_max50_1miss.npy\"\n",
    "\n",
    "t.load_dataset(path_essay, path_pos)\n",
    "t.train_model()\n",
    "t.test_model()\n",
    "\n",
    "data = EssayDataset(path_essay, path_pos)\n",
    "data.__getitem__(50)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
