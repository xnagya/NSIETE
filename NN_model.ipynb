{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Neural Network (NN) model implementation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Configuration of NN__ (Namespace variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Namespace(\n",
    "    # Dataset parameters\n",
    "    batch_size = 256,\n",
    "\n",
    "    # Model parameters\n",
    "    learning_rate = 0.001,\n",
    "    momentum = 0.9, \n",
    "    normalized_weight_init = False,\n",
    "    initial_bias = 0, \n",
    "    activation_fn = \"sigmoid\", # sigmoid | tanh | softsign | optimal\n",
    "    neurons_hidden1 = 25,\n",
    "    neurons_hidden2 = 25,\n",
    "    neurons_hidden3 = 25\n",
    ")\n",
    "\n",
    "def config_to_dict(ns: Namespace):\n",
    "    return vars(ns)\n",
    "\n",
    "def dict_to_config(d: dict):\n",
    "    return Namespace(**d)\n",
    "\n",
    "def normalize_dataset():\n",
    "    if (config.activation_fn == \"tanh\" or config.activation_fn == \"softsign\"): \n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Metrics__\n",
    "This class represnts all data collected during one training epoch. They are used to evaluate our NN model. This data is implmented as a class for easier integration with wandb.\n",
    "####  Metrics used:\n",
    "> Accuracy <br>\n",
    "> F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self, y_true, y_pred):\n",
    "\n",
    "        # Transorm predictions to classes\n",
    "        class_pred = binary_cutoff(y_pred, 0., 0.5, 1.)\n",
    "\n",
    "        y_list = []\n",
    "        for y in y_true:\n",
    "            y_list.append(y.item())\n",
    "\n",
    "        # Confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_list, class_pred).ravel()\n",
    "\n",
    "        # Calculate metrics\n",
    "        self.accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        self.precision = tp / (tp + fp)\n",
    "        self.recall = tp / (tp + fn)\n",
    "        self.f1_score = (2 * self.precision * self.recall) / (self.precision + self.recall)\n",
    "\n",
    "        self.accuracy = round(self.accuracy, 4) \n",
    "        self.precision = round(self.precision, 4) \n",
    "        self.recall = round(self.recall, 4) \n",
    "        self.f1_score = round(self.f1_score, 4) \n",
    "\n",
    "def binary_cutoff(predicted, class1, cutoff, class2):\n",
    "    actual_predictions = []\n",
    "\n",
    "    for y in predicted:\n",
    "        if (y < cutoff):\n",
    "            actual_predictions.append(class1)\n",
    "        else:\n",
    "            actual_predictions.append(class2)\n",
    "\n",
    "    return actual_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __FeedForward NN model__\n",
    "This class represents multilayer perceptron of feedforward NN used for binary classification on __Bioresponse__ dataset. It is stored as a variable in class __Trainer__ and contains functions for intilization of NN layers and intilization of their weight and biases. <br><br>\n",
    "> __Number of hidden layers:__ 4 <br>\n",
    "> __Activation between layers:__ sigmoid OR tanh OR softsign | in config (_activation_fn_)<br>\n",
    "> __Learinig rate:__ in config (_learning_rate_)<br>\n",
    "> __Intiliazation (weights):__ (_normalized_weight_init_ = TRUE) Xavier uniform distribution, (_normalized_weight_init_ = FALSE) uniform distribution<br>\n",
    "> __Intiliazation (bias):__ in config (_initial_bias_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create first hidden layer\n",
    "        layers = OrderedDict([\n",
    "            (\"flat\", nn.Flatten()), \n",
    "            (\"hidden1\", nn.Linear(input_size, config.neurons_hidden1, dtype = torch.float64))\n",
    "        ])\n",
    "        \n",
    "        # Add hidden layers with activation functions\n",
    "        match config.activation_fn:\n",
    "            case \"sigmoid\":\n",
    "                layers.update({\"sig1\" : nn.Sigmoid()})\n",
    "                layers.update({\"hidden2\" : nn.Linear(config.neurons_hidden1, config.neurons_hidden2, dtype = torch.float64)})\n",
    "                layers.update({\"sig2\" : nn.Sigmoid()})\n",
    "                layers.update({\"hidden3\" : nn.Linear(config.neurons_hidden2, config.neurons_hidden3, dtype = torch.float64)})\n",
    "                layers.update({\"sig3\" : nn.Sigmoid()})\n",
    "\n",
    "            case \"tanh\":\n",
    "                layers.update({\"tanh1\" : nn.Tanh()})\n",
    "                layers.update({\"hidden2\" : nn.Linear(config.neurons_hidden1, config.neurons_hidden2, dtype = torch.float64)})\n",
    "                layers.update({\"tanh2\" : nn.Tanh()})\n",
    "                layers.update({\"hidden3\" : nn.Linear(config.neurons_hidden2, config.neurons_hidden3, dtype = torch.float64)})\n",
    "                layers.update({\"tanh3\" : nn.Tanh()})\n",
    "\n",
    "\n",
    "            case \"softsign\":\n",
    "                layers.update({\"softs1\" : nn.Softsign()})\n",
    "                layers.update({\"hidden2\" : nn.Linear(config.neurons_hidden1, config.neurons_hidden2, dtype = torch.float64)})\n",
    "                layers.update({\"softs2\" : nn.Softsign()})\n",
    "                layers.update({\"hidden3\" : nn.Linear(config.neurons_hidden2, config.neurons_hidden3, dtype = torch.float64)})\n",
    "                layers.update({\"softs3\" : nn.Softsign()})\n",
    "                \n",
    "            case _ :\n",
    "                if (config.activation_fn != \"optimal\"):\n",
    "                    print(f\"ERROR: Wrong NN configuration: activation function = {config.activation_fn}\")\n",
    "\n",
    "                layers.update({\"relu1\" : nn.ReLU()})\n",
    "                layers.update({\"hidden2\" : nn.Linear(config.neurons_hidden1, config.neurons_hidden2, dtype = torch.float64)})\n",
    "                layers.update({\"softs2\" : nn.ReLU()})\n",
    "                layers.update({\"hidden3\" : nn.Linear(config.neurons_hidden2, config.neurons_hidden3, dtype = torch.float64)})\n",
    "                layers.update({\"relu3\" : nn.ReLU()})\n",
    "\n",
    "        # Output layer\n",
    "        layers.update({\"output\" : nn.Linear(config.neurons_hidden3, 1, dtype = torch.float64)})\n",
    "        \n",
    "        self.network = nn.Sequential(layers)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # Xavier uniform distribution\n",
    "        if config.normalized_weight_init:\n",
    "            for m in self.network.children():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    m.bias.data.fill_(config.initial_bias)\n",
    "\n",
    "        # Uniform distribution\n",
    "        else:\n",
    "            for m in self.network.children():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.uniform_(m.weight)\n",
    "                    m.bias.data.fill_(config.initial_bias)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        logits = self.network(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, config: Namespace, model: MultiLayerPerceptron):\n",
    "        self.cfg = config\n",
    "\n",
    "        # Select GPU device\n",
    "        self.device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "        print(f\"Using {self.device} device for training\")\n",
    "\n",
    "        # Move model to available device\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "        # Optimizer - Stochastic gradient descent\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.cfg.learning_rate, momentum=config.momentum)\n",
    "\n",
    "        # Loss function - Binary Cross Entropy\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        self.loss_train = []\n",
    "        self.loss_validate = []\n",
    "\n",
    "    # Create Data Loaders\n",
    "    def load_dataset(self, train_data, test_data):\n",
    "        self.train_data = DataLoader(train_data, batch_size=self.cfg.batch_size, shuffle=True)\n",
    "        self.test_data = DataLoader(test_data, batch_size=self.cfg.batch_size, shuffle=True)\n",
    "\n",
    "    def train(self, logger = None):\n",
    "        self.model.train()\n",
    "        self.loss_train = []\n",
    "\n",
    "        # Train model on each dataset batch (train_data)\n",
    "        for batch, (x, y) in enumerate(self.train_data):\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "            # Forward Pass - prediction and its error\n",
    "            pred = self.model(x)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "\n",
    "            # Backward Pass - update parameters (weights, bias)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.loss_train.append(loss)\n",
    "\n",
    "    def evaluate(self):\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "\n",
    "        self.loss_validate = []\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch, (x, y) in enumerate(self.test_data):\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "                # Forward Pass\n",
    "                pred = self.model(x)\n",
    "                loss = self.loss_fn(pred, y)\n",
    "\n",
    "                # Save batch loss\n",
    "                self.loss_validate.append(loss)\n",
    "                \n",
    "                # Save predictions and expected values\n",
    "                y_pred.extend(pred)\n",
    "                y_true.extend(y)\n",
    "\n",
    "        return Metrics(y_true, y_pred)\n",
    "    \n",
    "    def mean_loss(self):\n",
    "        loss_t = torch.mean(torch.FloatTensor(self.loss_train))\n",
    "        loss_v = torch.mean(torch.FloatTensor(self.loss_validate))\n",
    "        return loss_t, loss_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config.activation_fn = \"sigomid\"\n",
    "#mlp = MultiLayerPerceptron()\n",
    "#trainer = Trainer(config, mlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
