{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Neural Network (NN) model implementation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Configuration of NN__ (Namespace variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Namespace(\n",
    "    # Dataset parameters\n",
    "    batch_size = 128,\n",
    "\n",
    "    # Model parameters\n",
    "    learning_rate = 0.1,\n",
    "    normalized_weight_init = False,\n",
    "    initial_bias = 0, \n",
    "    activation_fn = \"sigmoid\", # sigmoid | tanh | softsign | optimal\n",
    "    neurons_hidden1 = 256,\n",
    "    neurons_hidden2 = 512,\n",
    "    neurons_hidden3 = 512,\n",
    "    neurons_hidden4 = 512\n",
    ")\n",
    "\n",
    "def config_to_dict(ns:Namespace):\n",
    "    return vars(config)\n",
    "\n",
    "def dict_to_config(d : dict):\n",
    "    return Namespace(**d)\n",
    "\n",
    "def normalize_dataset():\n",
    "    if (config.activation_fn == \"tanh\" or config.activation_fn == \"softsign\"): \n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Metrics__\n",
    "This class represnts all data collected during one training epoch. They are used to evaluate our NN model. This data is implmented as a class for easier integration with wandb.\n",
    "####  Metrics used:\n",
    "> Accuracy\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self, loss_per_batch, y_true, y_pred):\n",
    "        self.no_batches = len(loss_per_batch)\n",
    "        self.loss_values = loss_per_batch\n",
    "\n",
    "        no_predictions = len(y_true)\n",
    "\n",
    "        #print(len(y_pred))\n",
    "        #print(len(y_true))\n",
    "        self.accuracy = 0\n",
    "        self.f1_score = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __FeedForward NN model__\n",
    "This class represents multilayer perceptron of feedforward NN used for binary classification on __Bioresponse__ dataset. It is stored as a variable in class __Trainer__ and contains functions for intilization of NN layers and intilization of their weight and biases. <br><br>\n",
    "> __Number of hidden layers:__ 4 <br>\n",
    "> __Activation between layers:__ sigmoid OR tanh OR softsign | in config (_activation_fn_)<br>\n",
    "> __Learinig rate:__ in config (_learning_rate_)<br>\n",
    "> __Intiliazation (weights):__ (_normalized_weight_init_ = TRUE) Xavier uniform distribution, (_normalized_weight_init_ = FALSE) uniform distribution<br>\n",
    "> __Intiliazation (bias):__ in config (_initial_bias_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # create first hidden layer\n",
    "        layers = OrderedDict([\n",
    "            (\"hidden1\", nn.Linear(input_size, config.neurons_hidden1, dtype = torch.float64))\n",
    "        ])\n",
    "        \n",
    "        # Add other hidden + output layers with activation function\n",
    "        match config.activation_fn:\n",
    "            case \"sigmoid\":\n",
    "                layers.update({\"sig1\" : nn.Sigmoid()})\n",
    "                layers.update({\"hidden2\" : nn.Linear(config.neurons_hidden1, config.neurons_hidden2, dtype = torch.float64)})\n",
    "                layers.update({\"sig2\" : nn.Sigmoid()})\n",
    "                layers.update({\"hidden3\" : nn.Linear(config.neurons_hidden2, config.neurons_hidden3, dtype = torch.float64)})\n",
    "                layers.update({\"sig3\" : nn.Sigmoid()})\n",
    "                layers.update({\"hidden4\" : nn.Linear(config.neurons_hidden3, config.neurons_hidden4, dtype = torch.float64)})\n",
    "                layers.update({\"sig4\" : nn.Sigmoid()})\n",
    "                layers.update({\"output\" : nn.Linear(config.neurons_hidden4, 1, dtype = torch.float64)})\n",
    "                layers.update({\"sig5\" : nn.Sigmoid()})\n",
    "\n",
    "            case \"tanh\":\n",
    "                layers.update({\"tanh1\" : nn.Tanh()})\n",
    "                layers.update({\"hidden2\" : nn.Linear(config.neurons_hidden1, config.neurons_hidden2, dtype = torch.float64)})\n",
    "                layers.update({\"tanh2\" : nn.Tanh()})\n",
    "                layers.update({\"hidden3\" : nn.Linear(config.neurons_hidden2, config.neurons_hidden3, dtype = torch.float64)})\n",
    "                layers.update({\"tanh3\" : nn.Tanh()})\n",
    "                layers.update({\"hidden4\" : nn.Linear(config.neurons_hidden3, config.neurons_hidden4, dtype = torch.float64)})\n",
    "                layers.update({\"tanh4\" : nn.Tanh()})\n",
    "                layers.update({\"output\" : nn.Linear(config.neurons_hidden4, 1, dtype = torch.float64)})\n",
    "                layers.update({\"tanh5\" : nn.Tanh()})\n",
    "\n",
    "            case \"softsign\":\n",
    "                layers.update({\"softs1\" : nn.Softsign()})\n",
    "                layers.update({\"hidden2\" : nn.Linear(config.neurons_hidden1, config.neurons_hidden2, dtype = torch.float64)})\n",
    "                layers.update({\"softs2\" : nn.Softsign()})\n",
    "                layers.update({\"hidden3\" : nn.Linear(config.neurons_hidden2, config.neurons_hidden3, dtype = torch.float64)})\n",
    "                layers.update({\"softs3\" : nn.Softsign()})\n",
    "                layers.update({\"hidden4\" : nn.Linear(config.neurons_hidden3, config.neurons_hidden4, dtype = torch.float64)})\n",
    "                layers.update({\"softs4\" : nn.Softsign()})\n",
    "                layers.update({\"output\" : nn.Linear(config.neurons_hidden4, 1, dtype = torch.float64)})\n",
    "                layers.update({\"softs5\" : nn.Softsign()})\n",
    "                \n",
    "            case _ :\n",
    "                if (config.activation_fn != \"optimal\"):\n",
    "                    print(f\"ERROR: Wrong NN configuration: activation function = {config.activation_fn}\")\n",
    "\n",
    "                layers.update({\"relu1\" : nn.ReLU()})\n",
    "                layers.update({\"hidden2\" : nn.Linear(config.neurons_hidden1, config.neurons_hidden2, dtype = torch.float64)})\n",
    "                layers.update({\"relu2\" : nn.ReLU()})\n",
    "                layers.update({\"hidden3\" : nn.Linear(config.neurons_hidden2, config.neurons_hidden3, dtype = torch.float64)})\n",
    "                layers.update({\"relu3\" : nn.ReLU()})\n",
    "                layers.update({\"hidden4\" : nn.Linear(config.neurons_hidden3, config.neurons_hidden4, dtype = torch.float64)})\n",
    "                layers.update({\"relu4\" : nn.ReLU()})\n",
    "                layers.update({\"output\" : nn.Linear(config.neurons_hidden4, 1, dtype = torch.float64)})\n",
    "                layers.update({\"sigmoid\" : nn.Sigmoid()})\n",
    "\n",
    "        self.network = nn.Sequential(layers)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # Xavier uniform distribution\n",
    "        if config.normalized_weight_init:\n",
    "            for m in self.network.children():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    m.bias.data.fill_(config.initial_bias)\n",
    "\n",
    "        # Uniform distribution\n",
    "        else:\n",
    "            for m in self.network.children():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.uniform_(m.weight)\n",
    "                    m.bias.data.fill_(config.initial_bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.network(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, config: Namespace, model: MultiLayerPerceptron):\n",
    "        self.cfg = config\n",
    "\n",
    "        # Select GPU device\n",
    "        self.device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "        print(f\"Using {self.device} device for training\")\n",
    "\n",
    "        # Move model to available device\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "        # Optimizer - Stochastic gradient descent\n",
    "        self.optimizer = torch.optim.SGD(model.parameters(), lr=self.cfg.learning_rate) # momentum=0.9\n",
    "\n",
    "        # Loss function - Binary Cross Entropy\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "\n",
    "    # Create Data Loaders\n",
    "    def load_dataset(self, train_data, test_data):\n",
    "        self.train_data = DataLoader(train_data, batch_size=self.cfg.batch_size, shuffle=True)\n",
    "        self.test_data = DataLoader(test_data, batch_size=self.cfg.batch_size, shuffle=True)\n",
    "\n",
    "    # TODO cut-off based on predition\n",
    "    def cutoff_results(preditions):\n",
    "        return 0\n",
    "\n",
    "    def train(self):\n",
    "        # Train model on each dataset batch (train_data)\n",
    "        for batch, (x, y) in enumerate(self.train_data):\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "            # Forward Pass - prediction and its error\n",
    "            pred = self.model(x)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "\n",
    "            # Backward Pass - update parameters (weights, bias)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "    def evaluate(self):\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch, (x, y) in enumerate(self.test_data):\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "                # Forward Pass - prediction and its error\n",
    "                pred = self.model(x)\n",
    "                loss = self.loss_fn(pred, y)\n",
    "\n",
    "                losses.append(loss)\n",
    "\n",
    "                # Save preditions and expected values\n",
    "                y_pred.extend(pred)\n",
    "                y_true.extend(y)\n",
    "\n",
    "        return Metrics(losses, y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config.activation_fn = \"sigomid\"\n",
    "#mlp = MultiLayerPerceptron()\n",
    "#trainer = Trainer(config, mlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
