{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a14d939",
   "metadata": {},
   "source": [
    "# Dataset __Bioresponse__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc1c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce28cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('bioresponse.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc40191",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81791e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de8200",
   "metadata": {},
   "source": [
    "Each row in this data set represents a molecule. The first column contains experimental data describing an actual biological response; the molecule was seen to elicit this response (1), or not (0). The remaining columns represent molecular descriptors (d1 through d1776), these are calculated properties that can capture some of the characteristics of the molecule - for example size, shape, or elemental constitution. The \"target\" column is the biological response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846b82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['target']) # Input features (molecular descriptors)\n",
    "Y = data['target'] # Target variable (biological response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef59300",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78533dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target value distribution\n",
    "def target_distribution(data):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.countplot(x='target', data=data, palette='pastel')\n",
    "    plt.title('Distribution of the target value')\n",
    "    plt.xlabel('Target Variable')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "# Boxplots for the first N descriptors\n",
    "def firstN_descriptors(data, num):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(data=data.iloc[:, 1:num])\n",
    "    plt.title('Boxplot of Molecular Descriptors (d1-d10)')\n",
    "    plt.xlabel('Descriptor')\n",
    "    plt.ylabel('Value')\n",
    "    plt.show()\n",
    "\n",
    "# Visualisation of the relationship between the first molecular descriptor (X1) and the target variable\n",
    "def descriptor_target_relationship(data, idx):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(x=data['target'], y=data.iloc[:, idx], color='lightgreen')\n",
    "    plt.title('Relationship between the first descriptor and Target Variable')\n",
    "    plt.xlabel('Target Variable')\n",
    "    plt.ylabel('X')\n",
    "    plt.show()\n",
    "\n",
    "def heatmaps_corr(data):\n",
    "    X = data.drop(columns=['target']) # Input features (molecular descriptors)\n",
    "    Y = data['target'] # Target variable (biological response)\n",
    "\n",
    "    correlation_X = X.corr()  # Correlation among molecular descriptors\n",
    "    correlation_Y = X.apply(lambda x: x.corr(Y))  # Correlation between each molecular descriptor and the target variable\n",
    "\n",
    "    # Heatmap for correlation among molecular descriptors\n",
    "    sns.heatmap(correlation_X, cmap='coolwarm', annot=False, ax=axes[0])\n",
    "    axes[0].set_title('Correlation Heatmap - Molecular Descriptors')\n",
    "    axes[0].set_xlabel('Molecular Descriptors')\n",
    "    axes[0].set_ylabel('Molecular Descriptors')\n",
    "\n",
    "    # Heatmap for correlation between molecular descriptors and target variable\n",
    "    sns.heatmap(correlation_Y.to_frame().transpose(), cmap='coolwarm', annot=True, fmt=\".2f\", ax=axes[1])\n",
    "    axes[1].set_title('Correlation Heatmap - Molecular Descriptors vs. Target Variable')\n",
    "    axes[1].set_xlabel('Molecular Descriptors')\n",
    "    axes[1].set_ylabel('Target Variable (Y)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685ea8c1",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6d6f21",
   "metadata": {},
   "source": [
    "#### Merging descriptors with similar correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation_threshold = treshold for correlation to merge descriptors (bigger corr => merget descriptors) \n",
    "def merge_descriptors(data, correlation_threshold = 0.5):\n",
    "    X = data.drop(columns=['target']) # Input features (molecular descriptors)\n",
    "    Y = data['target'] # Target variable (biological response)\n",
    "\n",
    "    correlation_Y = X.apply(lambda x: x.corr(Y))\n",
    "    correlation_matrix = data.corr()\n",
    "    merged_descriptors = set()\n",
    "\n",
    "    # Iterate over the correlation matrix to identify pairs of descriptors with similar correlation\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            if abs(correlation_matrix.iloc[i, j]) >= correlation_threshold:\n",
    "                # Add correlated descriptors to the set\n",
    "                merged_descriptors.add((correlation_matrix.columns[i], correlation_matrix.columns[j]))\n",
    "\n",
    "    merged_data = data.copy()\n",
    "\n",
    "    # Merge descriptors\n",
    "    for descriptor_pair in merged_descriptors:\n",
    "        # Check if both descriptors exist in the dataset\n",
    "        if all(descriptor in merged_data.columns for descriptor in descriptor_pair):\n",
    "            merged_descriptor_name = '_'.join(descriptor_pair)\n",
    "            merged_data[merged_descriptor_name] = (data[descriptor_pair[0]] + data[descriptor_pair[1]]) / 2\n",
    "            merged_data.drop(list(descriptor_pair), axis=1, inplace=True)\n",
    "\n",
    "#     print(\"Information about Merged DataFrame:\")\n",
    "#     print(merged_data.info())\n",
    "\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37daf77",
   "metadata": {},
   "source": [
    "#### Linear correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7055a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_selection_original(data, correlation_threshold = 0.2):\n",
    "    X = data.drop(columns=['target']) # Input features (molecular descriptors)\n",
    "    Y = data['target'] # Target variable (biological response)\n",
    "\n",
    "    correlation_Y = X.apply(lambda x: x.corr(Y))\n",
    "    selected_features = correlation_Y[correlation_Y >= correlation_threshold].index.tolist()\n",
    "\n",
    "    selected_data = data[selected_features]\n",
    "\n",
    "    print(\"Selected Features from original data:\")\n",
    "    print(selected_features)\n",
    "    \n",
    "    return pd.DataFrame(data=data[selected_features + ['target']])\n",
    "\n",
    "def correlation_selection_merged(data, correlation_threshold = 0.2):\n",
    "    merged_data = merge_descriptors(data)\n",
    "            \n",
    "    X_m = merged_data.drop(columns=['target']) # Input features (molecular descriptors)\n",
    "    Y_m = merged_data['target'] # Target variable (biological response)\n",
    "\n",
    "    correlation_Y_m = X_m.apply(lambda x: x.corr(Y_m))\n",
    "    selected_features_m = correlation_Y_m[correlation_Y_m >= correlation_threshold].index.tolist()\n",
    "\n",
    "    selected_data_m = merged_data[selected_features_m]\n",
    "\n",
    "    print(\"Selected Features from merged data:\")\n",
    "    print(selected_features_m)\n",
    "    \n",
    "    return pd.DataFrame(data=merged_data[selected_features_m + ['target']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1ddb75",
   "metadata": {},
   "source": [
    "As you can see, the linear correlation is not too high between the target_value and the descriptors, so I have to experiment fith another feature selection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b3738b",
   "metadata": {},
   "source": [
    "#### Tree based selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaab513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_based_original(data, n_estimators=100, top_n=50, random_state=42):\n",
    "    X = data.drop(columns=['target'])\n",
    "    Y = data['target']\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "    rf.fit(X, Y)\n",
    "\n",
    "    feature_importances = rf.feature_importances_\n",
    "    importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "    top_features = importance_df['Feature'].head(top_n).tolist()\n",
    "    \n",
    "    print(\"Top features:\")\n",
    "    top_features[:5]\n",
    "    \n",
    "    return pd.DataFrame(data=data[top_features + ['target']]) \n",
    "\n",
    "def tree_based_merged(data, n_estimators=100, top_n=50, random_state=42):\n",
    "    merged_data = merge_descriptors(data)\n",
    "            \n",
    "    X_m = merged_data.drop(columns=['target'])\n",
    "    Y_m = merged_data['target']\n",
    "    \n",
    "    rf_m = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "    rf_m.fit(X_m, Y_m)\n",
    "\n",
    "    feature_importances_m = rf_m.feature_importances_\n",
    "    importance_df_m = pd.DataFrame({'Feature': X_m.columns, 'Importance': feature_importances_m})\n",
    "    importance_df_m = importance_df_m.sort_values(by='Importance', ascending=False)\n",
    "    top_features_m = importance_df_m['Feature'].head(top_n).tolist()\n",
    "    \n",
    "    print(\"Top featuresmerged:\")\n",
    "    top_features_m[:5]\n",
    "    \n",
    "    return pd.DataFrame(data=merged_data[top_features_m + ['target']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1533156f",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9027e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def pca(data, n_components=50):\n",
    "    X = data.drop(columns=['target'])\n",
    "    y = data['target']\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    pca.fit(X_scaled)\n",
    "\n",
    "    X_pca = pca.transform(X_scaled)\n",
    "\n",
    "    pca_scaler = StandardScaler()\n",
    "    X_pca_normalized = pca_scaler.fit_transform(X_pca)\n",
    "\n",
    "    principal_components_df = pd.DataFrame(data=X_pca_normalized, columns=[f'PC{i+1}_normalized' for i in range(n_components)])\n",
    "    \n",
    "    principal_components_df['target'] = data['target']\n",
    "    \n",
    "    return principal_components_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c849bf7a",
   "metadata": {},
   "source": [
    "### Polynomial feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ffd59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_float32 = data.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f450548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# # Feature Engineering\n",
    "# poly = PolynomialFeatures(degree=1.5, interaction_only=True, include_bias=False)\n",
    "# X_poly = poly.fit_transform(X)\n",
    "\n",
    "# # Convert the polynomial feature matrix to a DataFrame\n",
    "# X_poly_df = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(X.columns))\n",
    "\n",
    "# # Concatenate the original features with the polynomial features\n",
    "# X_combined = pd.concat([X, X_poly_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3394f5cf",
   "metadata": {},
   "source": [
    "This method occures memory problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d7003",
   "metadata": {},
   "source": [
    "## Summary of the feature selection\n",
    "We created a feature selection according to linear correlation for original data, and tried to merge the descriptors with similar correlation value in relation with the target value. [selected_features, selected_features_m] \n",
    "\n",
    "\n",
    "We also selected features using tree based feature selection method using the original and the merged data. These lists represents the top 50 features. [top_features, top_features_m]\n",
    "\n",
    "\n",
    "We tried polynomial feature selection as well, but in this case we had memory problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d2623",
   "metadata": {},
   "source": [
    "#### Selected features dataframe export functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4edaea",
   "metadata": {},
   "source": [
    "You can get the dataframes, which are containing the specific selected features with calling: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ad2b8",
   "metadata": {},
   "source": [
    "correlation_selection_original(data, correlation_threshold), correlation_selection_merged(data, correlation_threshold), tree_based_original(data, n_estimators, top_n, random_state), tree_based_merged(data, n_estimators, top_n, random_state), pca(data, n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01bed4e",
   "metadata": {},
   "source": [
    "Calling functions and viewing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9532174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_data = correlation_selection_merged(data)\n",
    "#new_data = tree_based_merged(data)\n",
    "#new_data = pca(data, n_components = 10)\n",
    "\n",
    "#new_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
