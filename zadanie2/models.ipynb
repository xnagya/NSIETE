{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Neural network models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torchvision import models\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pvtv2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net Network\n",
    "\n",
    "Consists of:\n",
    "- Encoder Block\n",
    "- Bridge layer (conv2D)\n",
    "- Decoder Block\n",
    "- Output Layer (conv2D with channel = numer of classes)\n",
    "\n",
    "Arguments:\n",
    "- channels_in = Number of input image channels (usually 3 - RGB)\n",
    "- Bridge layer (conv2D)\n",
    "- Decoder Block\n",
    "- Output Layer (conv2D with channel = numer of classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_layer(channels_in, channels_out, kernel_size, padding, stride, dilation):\n",
    "    layer = nn.ModuleList()\n",
    "\n",
    "    conv2d = nn.Conv2d(\n",
    "        in_channels = channels_in, \n",
    "        out_channels = channels_out, \n",
    "        kernel_size = kernel_size, \n",
    "        stride = stride, \n",
    "        padding = padding, \n",
    "        dilation = dilation\n",
    "    )\n",
    "    torch.nn.init.kaiming_normal_(conv2d.weight)\n",
    "    \n",
    "    layer.append(conv2d)\n",
    "    layer.append(nn.ReLU())\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Block(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, params: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        # Config parameters\n",
    "        try:\n",
    "            block_width = params['block_width']\n",
    "            kernel_size = params['kernel_size']\n",
    "            padding = params['padding']\n",
    "            stride = params['stride']\n",
    "            dilation = params['dilation']\n",
    "            pool_type = params['pool_type']\n",
    "            pool_size = params['pool_kernel_size']\n",
    "        except KeyError as e:\n",
    "            raise Exception(f'Parameter \"{e.args[0]}\" NOT found!')\n",
    "        \n",
    "        # Add first Conv2D layer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.extend(conv2d_layer(channels_in, channels_out, kernel_size, padding, stride, dilation))\n",
    "\n",
    "        # Add other Conv2D layers\n",
    "        for _ in range(block_width - 1):\n",
    "            self.layers.extend(conv2d_layer(channels_out, channels_out, kernel_size, padding, stride, dilation))\n",
    "\n",
    "        # Add pooling layer\n",
    "        match pool_type:\n",
    "            case 'max':\n",
    "                self.pooling = nn.MaxPool2d(kernel_size=pool_size, stride=pool_size)\n",
    "            case 'avg':\n",
    "                self.pooling = nn.AvgPool2d(kernel_size=pool_size, stride=pool_size)\n",
    "\n",
    "\n",
    "    def forward_noSkip(self, x):\n",
    "        # Convolution\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Pooling    \n",
    "        x = self.pooling(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def forward_skip(self, x):\n",
    "        # Convolution\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Pooling, convolution output\n",
    "        return (self.pooling(x), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Block(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, padding_t, params: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        # Config parameters\n",
    "        try:\n",
    "            block_width = params['block_width']\n",
    "            kernel_size = params['kernel_size']\n",
    "            padding = params['padding']\n",
    "            stride = params['stride']\n",
    "            dilation = params['dilation']\n",
    "            pool_size = params['pool_kernel_size']\n",
    "            skip_features = params['skip_features']\n",
    "        except KeyError as e:\n",
    "            raise Exception(f'Parameter \"{e.args[0]}\" NOT found!')\n",
    "\n",
    "        # Add transpose convolution\n",
    "        channel_out_convT = math.floor(channels_in / 2)\n",
    "\n",
    "        self.convT = nn.ConvTranspose2d (\n",
    "            in_channels = channels_in, \n",
    "            out_channels = channel_out_convT, \n",
    "            kernel_size = pool_size, \n",
    "            stride = pool_size, \n",
    "            output_padding = padding_t\n",
    "        )\n",
    "        \n",
    "        match skip_features:\n",
    "            case \"none\":\n",
    "                channels_in = channel_out_convT\n",
    "                \n",
    "            case \"concat\":\n",
    "                # Half of channels are from convT, other half from enc features\n",
    "                # channels_in = 2 * channel_out_convT\n",
    "                pass\n",
    "\n",
    "        # Add first Conv2D layer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.extend(conv2d_layer(channels_in, channels_out, kernel_size, padding, stride, dilation))\n",
    "\n",
    "        # Add other Conv2D layers\n",
    "        for _ in range(block_width - 1):\n",
    "            self.layers.extend(conv2d_layer(channels_out, channels_out, kernel_size, padding, stride, dilation))\n",
    "\n",
    "    def forward_noSkip(self, x):\n",
    "        # Transposed convolution\n",
    "        x = self.convT(x)\n",
    "\n",
    "        # Convolution\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_skip(self, x, enc_feature):\n",
    "        # Transposed convolution\n",
    "        x = self.convT(x)\n",
    "\n",
    "        # Copy features\n",
    "        x = torch.cat([enc_feature, x], dim = 1)\n",
    "\n",
    "        # Convolution\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_Net(nn.Module):\n",
    "    def __init__(self, channels_in, num_classes, params: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "\n",
    "        # Config parameters\n",
    "        try:\n",
    "            channel_mul = params['channel_mul']\n",
    "            depth = params['network_depth']\n",
    "            channels_out = params['channels_out_init']\n",
    "            padding_convT = deepcopy(params['padding_convT'])\n",
    "            self.skip_features = params['skip_features']\n",
    "            if (len(padding_convT) != depth):\n",
    "                raise Exception(f\"Padding of Transposed convolution has length {len(padding_convT)}, but should be {depth}!\")\n",
    "            \n",
    "        except KeyError as e:\n",
    "            raise Exception(f'Parameter \"{e.args[0]}\" NOT found!')\n",
    "        \n",
    "        # Create encoder\n",
    "        self.encoders = nn.ModuleList()\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.encoders.append(Encoder_Block(channels_in, channels_out, params))\n",
    "            channels_in = channels_out\n",
    "            channels_out = math.floor(channels_out * channel_mul)\n",
    "\n",
    "        # Create bridge as Conv2D layer\n",
    "        last_encoder = Encoder_Block(channels_in, channels_out, params)\n",
    "        self.bridge = last_encoder.layers[0]\n",
    "\n",
    "        channels_in = channels_out\n",
    "        channels_out = math.floor(channels_in / channel_mul)\n",
    "\n",
    "        # Create decoder\n",
    "        self.decoders = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(depth) :\n",
    "            paddingT = padding_convT.pop(0)\n",
    "            self.decoders.append(Decoder_Block(channels_in, channels_out, paddingT, params))\n",
    "            channels_in = channels_out\n",
    "            channels_out = math.floor(channels_out / channel_mul)\n",
    "\n",
    "        # Create output layer (1x1 convolution, return logits)\n",
    "        self.output_layer = nn.Conv2d(\n",
    "            in_channels = channels_in, \n",
    "            out_channels = num_classes, \n",
    "            kernel_size = 1\n",
    "        )\n",
    "\n",
    "        match self.skip_features:\n",
    "            case \"none\":\n",
    "                print(\"This U-Net does NOT skip any connections.\")\n",
    "            case \"concat\":\n",
    "                print(\"This U-Net skips connections using concatenation.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        match self.skip_features:\n",
    "            # No skipping\n",
    "            case \"none\":\n",
    "                # Encoding\n",
    "                for enc in self.encoders:\n",
    "                    x = enc.forward_noSkip(x)\n",
    "\n",
    "                # Bridge between encoder and decoder\n",
    "                x = self.bridge(x)\n",
    "\n",
    "                # Decoding\n",
    "                for dec in self.decoders:\n",
    "                    x = dec.forward_noSkip(x)\n",
    "\n",
    "            # Cat tensors => features + pooled output\n",
    "            case \"concat\":\n",
    "                # Features created during enconding\n",
    "                features = []\n",
    "\n",
    "                # Encoding\n",
    "                for enc in self.encoders:\n",
    "                    x, enc_feature = enc.forward_skip(x)\n",
    "                    features.append(enc_feature)\n",
    "\n",
    "                # Bridge between encoder and decoder\n",
    "                x = self.bridge(x)\n",
    "\n",
    "                # Decoding\n",
    "                for dec in self.decoders:\n",
    "                    enc_output = features.pop()\n",
    "                    x = dec.forward_skip(x, enc_output)  \n",
    "\n",
    "                del features\n",
    "\n",
    "        # Output as logits\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_output_shape(height, width, conv2d: nn.Module):\n",
    "    height = ((height + (2 * conv2d.padding[0]) - (conv2d.dilation[0] * (conv2d.kernel_size[0] - 1)) - 1) / conv2d.stride[0]) + 1\n",
    "    height = math.floor(height)\n",
    "    \n",
    "    width = ((width + (2 * conv2d.padding[1]) - (conv2d.dilation[1] * (conv2d.kernel_size[1] - 1)) - 1) / conv2d.stride[1]) + 1\n",
    "    width = math.floor(width)\n",
    "\n",
    "    return (height, width, conv2d.out_channels)\n",
    "\n",
    "def pool_output_shape(height, width, conv2d: nn.Module):\n",
    "    height = ((height + (2 * conv2d.padding) - (conv2d.dilation * (conv2d.kernel_size - 1)) - 1) / conv2d.stride) + 1\n",
    "    height = math.floor(height)\n",
    "    \n",
    "    width = ((width + (2 * conv2d.padding) - (conv2d.dilation * (conv2d.kernel_size - 1)) - 1) / conv2d.stride) + 1\n",
    "    width = math.floor(width)\n",
    "\n",
    "    return (height, width)\n",
    "\n",
    "def convT_output_shape(height, width, convT: nn.Module):\n",
    "    height = (height - 1) * convT.stride[0] - (2 * convT.padding[0]) + (convT.dilation[0] * (convT.kernel_size[0] - 1)) + convT.output_padding[0] + 1\n",
    "    \n",
    "    width = (width - 1) * convT.stride[1] - (2 * convT.padding[1]) + (convT.dilation[1] * (convT.kernel_size[1] - 1)) + convT.output_padding[1] + 1\n",
    "    \n",
    "    return (height, width, convT.out_channels)\n",
    "\n",
    "\n",
    "def output_shapes(net : U_Net, height, width):\n",
    "    h = height\n",
    "    w = width\n",
    "    depth = len(net.encoders) + 1\n",
    "\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"CNN with depth = {depth}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    i = 1\n",
    "    for enc in net.encoders:\n",
    "        print(f\"||Layer {i} - encoders||\")\n",
    "\n",
    "        for l in enc.layers:\n",
    "            if not isinstance(l, nn.ReLU):\n",
    "                (h,w,c) = conv2d_output_shape(h, w, l)\n",
    "                print (f\"Conv2D -- H = {h} | W = {w} | C = {c}\")\n",
    "\n",
    "        (h,w) = pool_output_shape(h, w, enc.pooling)\n",
    "        print (f\"Pooling -- H = {h} | W = {w} | C = {c}\") \n",
    "\n",
    "        print()\n",
    "        i += 1\n",
    "            \n",
    "    print(f\"||Layer {i} - bridge||\")\n",
    "    (h,w,c) = conv2d_output_shape(h, w, net.bridge)\n",
    "    print (f\"H = {h} | W = {w} | C = {c}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    for dec in net.decoders:\n",
    "        print(f\"||Layer {i} - decoders||\")\n",
    "\n",
    "        (h,w,c) = convT_output_shape(h, w, dec.convT)\n",
    "        print (f\"Transpose -- H = {h} | W = {w} | C = {c}\")   \n",
    "\n",
    "        for l in dec.layers:\n",
    "            if not isinstance(l, nn.ReLU):\n",
    "                (h,w,c) = conv2d_output_shape(h, w, l)\n",
    "                print (f\"Conv2D -- H = {h} | W = {w} | C = {c}\")   \n",
    "\n",
    "        print()\n",
    "        i -= 1\n",
    "\n",
    "\n",
    "    print(f\"||Layer {i} - output||\")\n",
    "    (h,w,c) = conv2d_output_shape(h, w, net.output_layer)\n",
    "    print (f\"H = {h} | W = {w} | C = {c}\")\n",
    "    print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        middle_channels = math.floor(in_planes // ratio)\n",
    "\n",
    "        self.fc1 = nn.Conv2d(in_planes, middle_channels, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(middle_channels, in_planes, 1, bias=False)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes,\n",
    "                              kernel_size=kernel_size, stride=stride,\n",
    "                              padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class SDI(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(channel, channel, kernel_size=3, stride=1, padding=1) for _ in range(4)])\n",
    "\n",
    "    def forward(self, xs, anchor):\n",
    "        ans = torch.ones_like(anchor)\n",
    "        target_size = anchor.shape[-1]\n",
    "\n",
    "        for i, x in enumerate(xs):\n",
    "            if x.shape[-1] > target_size:\n",
    "                x = F.adaptive_avg_pool2d(x, (target_size, target_size))\n",
    "            elif x.shape[-1] < target_size:\n",
    "                x = F.interpolate(x, size=(target_size, target_size),\n",
    "                                      mode='bilinear', align_corners=True)\n",
    "\n",
    "            ans = ans * self.convs[i](x)\n",
    "\n",
    "        return ans\n",
    "    \n",
    "def pad_dimensions(desired: torch.tensor, wrong: torch.tensor):\n",
    "    if desired.ndimension() != wrong.ndimension():\n",
    "        return wrong\n",
    "    \n",
    "    padding = ()\n",
    "    \n",
    "    for i in reversed(range(desired.ndimension())):\n",
    "        diff = desired.size(i) - wrong.size(i)\n",
    "        left = math.floor(diff / 2)\n",
    "        right = math.ceil(diff / 2)\n",
    "        padding += (left, right)\n",
    "\n",
    "    return F.pad(wrong, padding, \"constant\", 0)\n",
    "    \n",
    "\n",
    "class UNetV2(nn.Module):\n",
    "    def __init__(self, n_classes, params: dict):\n",
    "        super().__init__()\n",
    "        # Config parameters\n",
    "        try:\n",
    "            channels = params['SDI_channels']\n",
    "            ratio_attention = params['channel_att_ratio']\n",
    "        except KeyError as e:\n",
    "            raise Exception(f'Parameter \"{e.args[0]}\" NOT found!')\n",
    "        \n",
    "        self.deep_supervision = False\n",
    "\n",
    "        self.encoder = pvt_v2_b2()\n",
    "\n",
    "        self.ca_1 = ChannelAttention(64, ratio_attention)\n",
    "        self.sa_1 = SpatialAttention()\n",
    "\n",
    "        self.ca_2 = ChannelAttention(128, ratio_attention)\n",
    "        self.sa_2 = SpatialAttention()\n",
    "\n",
    "        self.ca_3 = ChannelAttention(320, ratio_attention)\n",
    "        self.sa_3 = SpatialAttention()\n",
    "\n",
    "        self.ca_4 = ChannelAttention(512, ratio_attention)\n",
    "        self.sa_4 = SpatialAttention()\n",
    "\n",
    "        self.Translayer_1 = BasicConv2d(64, channels, 1)\n",
    "        self.Translayer_2 = BasicConv2d(128, channels, 1)\n",
    "        self.Translayer_3 = BasicConv2d(320, channels, 1)\n",
    "        self.Translayer_4 = BasicConv2d(512, channels, 1)\n",
    "\n",
    "        self.sdi_1 = SDI(channels)\n",
    "        self.sdi_2 = SDI(channels)\n",
    "        self.sdi_3 = SDI(channels)\n",
    "        self.sdi_4 = SDI(channels)\n",
    "\n",
    "        self.seg_outs = nn.ModuleList([\n",
    "            nn.Conv2d(channels, n_classes, 1, 1) for _ in range(4)])\n",
    "\n",
    "        self.deconv2 = nn.ConvTranspose2d(channels, channels, kernel_size=4, stride=2, padding=1,\n",
    "                                          bias=False)\n",
    "        self.deconv3 = nn.ConvTranspose2d(channels, channels, kernel_size=4, stride=2,\n",
    "                                          padding=1, bias=False)\n",
    "        self.deconv4 = nn.ConvTranspose2d(channels, channels, kernel_size=4, stride=2,\n",
    "                                          padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seg_outs = []\n",
    "        f1, f2, f3, f4 = self.encoder(x) \n",
    "\n",
    "        f1 = self.ca_1(f1) * f1\n",
    "        f1 = self.sa_1(f1) * f1\n",
    "        f1 = self.Translayer_1(f1)\n",
    "\n",
    "        f2 = self.ca_2(f2) * f2\n",
    "        f2 = self.sa_2(f2) * f2\n",
    "        f2 = self.Translayer_2(f2)\n",
    "\n",
    "        f3 = self.ca_3(f3) * f3\n",
    "        f3 = self.sa_3(f3) * f3\n",
    "        f3 = self.Translayer_3(f3)\n",
    "\n",
    "        f4 = self.ca_4(f4) * f4\n",
    "        f4 = self.sa_4(f4) * f4\n",
    "        f4 = self.Translayer_4(f4)\n",
    "\n",
    "        f41 = self.sdi_4([f1, f2, f3, f4], f4)\n",
    "        f31 = self.sdi_3([f1, f2, f3, f4], f3)\n",
    "        f21 = self.sdi_2([f1, f2, f3, f4], f2)\n",
    "        f11 = self.sdi_1([f1, f2, f3, f4], f1)\n",
    "\n",
    "        seg_outs.append(self.seg_outs[0](f41))\n",
    "\n",
    "        y = self.deconv2(f41)\n",
    "        y = pad_dimensions(f31, y)\n",
    "        y += f31\n",
    "        seg_outs.append(self.seg_outs[1](y))\n",
    "\n",
    "\n",
    "        y = self.deconv3(y)\n",
    "        y = pad_dimensions(f21, y)\n",
    "        y += f21\n",
    "        seg_outs.append(self.seg_outs[2](y))\n",
    "\n",
    "\n",
    "        y = self.deconv4(y)\n",
    "        y = pad_dimensions(f11, y)\n",
    "        y += f11\n",
    "        seg_outs.append(self.seg_outs[3](y))\n",
    "\n",
    "        for i, o in enumerate(seg_outs):\n",
    "            seg_outs[i] = F.interpolate(o, scale_factor=4, mode='bilinear')\n",
    "\n",
    "        if self.deep_supervision:\n",
    "            return seg_outs[::-1]\n",
    "        else:\n",
    "            return seg_outs[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
