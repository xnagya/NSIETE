{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torchvision import models\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net Network\n",
    "num_classes = Number of classes expecteed from output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_layer(channels_in, channels_out, kernel_size, padding, stride, dilation):\n",
    "    layer = nn.ModuleList()\n",
    "  \n",
    "    layer.append(nn.Conv2d(\n",
    "        in_channels = channels_in, \n",
    "        out_channels = channels_out, \n",
    "        kernel_size = kernel_size, \n",
    "        stride = stride, \n",
    "        padding = padding, \n",
    "        dilation = dilation\n",
    "        )\n",
    "    )\n",
    "    layer.append(nn.ReLU())\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Block(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, params: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        # Config parameters\n",
    "        try:\n",
    "            block_width = params['block_width']\n",
    "            kernel_size = params['kernel_size']\n",
    "            padding = params['padding']\n",
    "            stride = params['stride']\n",
    "            dilation = params['dilation']\n",
    "            pool_type = params['pool_type']\n",
    "            pool_size = params['pool_kernel_size']\n",
    "        except KeyError as e:\n",
    "            raise Exception(f'Parameter \"{e.args[0]}\" NOT found!')\n",
    "        \n",
    "        # Add first Conv2D layer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.extend(conv2d_layer(channels_in, channels_out, kernel_size, padding, stride, dilation))\n",
    "\n",
    "        # Add other Conv2D layers\n",
    "        for _ in range(block_width - 1):\n",
    "            self.layers.extend(conv2d_layer(channels_out, channels_out, kernel_size, padding, stride, dilation))\n",
    "\n",
    "        # Add pooling layer\n",
    "        match pool_type:\n",
    "            case 'max':\n",
    "                self.pooling = nn.MaxPool2d(kernel_size=pool_size, stride=pool_size)\n",
    "            case 'avg':\n",
    "                self.pooling = nn.AvgPool2d(kernel_size=pool_size, stride=pool_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Pooling\n",
    "        return (self.pooling(x), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Block(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, padding_t, params: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        # Config parameters\n",
    "        try:\n",
    "            block_width = params['block_width']\n",
    "            kernel_size = params['kernel_size']\n",
    "            padding = params['padding']\n",
    "            stride = params['stride']\n",
    "            dilation = params['dilation']\n",
    "            pool_size = params['pool_kernel_size']\n",
    "        except KeyError as e:\n",
    "            raise Exception(f'Parameter \"{e.args[0]}\" NOT found!')\n",
    "\n",
    "        # Add transpose convolution\n",
    "        self.convT = nn.ConvTranspose2d (\n",
    "            in_channels = channels_in, \n",
    "            out_channels = math.floor(channels_in / 2), \n",
    "            kernel_size = pool_size, \n",
    "            stride = pool_size, \n",
    "            output_padding = padding_t\n",
    "        )\n",
    "\n",
    "        # Add first Conv2D layer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.extend(conv2d_layer(channels_in, channels_out, kernel_size, padding, stride, dilation))\n",
    "\n",
    "        # Add other Conv2D layers\n",
    "        for _ in range(block_width - 1):\n",
    "            self.layers.extend(conv2d_layer(channels_out, channels_out, kernel_size, padding, stride, dilation))\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_features):\n",
    "        # Transposed convolution\n",
    "        x = self.convT(x)\n",
    "\n",
    "        # Copy and crop features\n",
    "        w = x.size(-1)\n",
    "        c = math.floor((enc_features.size(-1) - w) / 2)\n",
    "\n",
    "        print(x.size())\n",
    "        print(x.size(dim=1))\n",
    "        \n",
    "        print(enc_features.size())\n",
    "        print(enc_features.size(dim=1))\n",
    "        #enc_features = enc_features[:, :, c:c+w , c:c+w]\n",
    "        x = torch.cat([enc_features, x], dim = 1)\n",
    "\n",
    "        # Convolution\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_Net(nn.Module):\n",
    "    def __init__(self, channels_in, num_classes, params: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        # Config parameters\n",
    "        try:\n",
    "            channel_mul = params['channel_mul']\n",
    "            depth = params['network_depth']\n",
    "            channels_out = params['channels_out_init']\n",
    "            padding_convT = params['padding_convT']\n",
    "        except KeyError as e:\n",
    "            raise Exception(f'Parameter \"{e.args[0]}\" NOT found!')\n",
    "        \n",
    "        # Create encoder\n",
    "        self.encoders = nn.ModuleList()\n",
    "\n",
    "        for _ in range(depth) :\n",
    "            self.encoders.append(Encoder_Block(channels_in, channels_out, params))\n",
    "            channels_in = channels_out\n",
    "            channels_out = math.floor(channels_out * channel_mul)\n",
    "\n",
    "        # Create bridge as Conv2D layer\n",
    "        last_encoder = Encoder_Block(channels_in, channels_out, params)\n",
    "        self.bridge = last_encoder.layers[0]\n",
    "\n",
    "        channels_in = channels_out\n",
    "        channels_out = math.floor(channels_in / channel_mul)\n",
    "\n",
    "        # Create decoder\n",
    "        self.decoders = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(depth) :\n",
    "            paddingT = padding_convT.pop(0)\n",
    "            self.decoders.append(Decoder_Block(channels_in, channels_out, paddingT, params))\n",
    "            channels_in = channels_out\n",
    "            channels_out = math.floor(channels_out / channel_mul)\n",
    "\n",
    "        # Create output layer (1x1 convolution, return logits)\n",
    "        self.output_layer = nn.Conv2d(\n",
    "            in_channels = channels_in, \n",
    "            out_channels = num_classes, \n",
    "            kernel_size = 1\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Features created during enconding\n",
    "        features = []\n",
    "\n",
    "        # Encoding\n",
    "        for enc in self.encoders:\n",
    "            x, conv_output = enc(x)\n",
    "            features.append(conv_output)\n",
    "\n",
    "        # Bridge between encoder and decoder\n",
    "        x = self.bridge(x)\n",
    "\n",
    "        # Decoding\n",
    "        for dec in self.decoders:\n",
    "            enc_output = features.pop()\n",
    "            x = dec(x, enc_output)\n",
    "\n",
    "        # Return logits\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_output_shape(height, width, conv2d: nn.Module):\n",
    "    height = ((height + (2 * conv2d.padding[0]) - (conv2d.dilation[0] * (conv2d.kernel_size[0] - 1)) - 1) / conv2d.stride[0]) + 1\n",
    "    height = math.floor(height)\n",
    "    \n",
    "    width = ((width + (2 * conv2d.padding[1]) - (conv2d.dilation[1] * (conv2d.kernel_size[1] - 1)) - 1) / conv2d.stride[1]) + 1\n",
    "    width = math.floor(width)\n",
    "\n",
    "    return (height, width, conv2d.out_channels)\n",
    "\n",
    "def pool_output_shape(height, width, conv2d: nn.Module):\n",
    "    height = ((height + (2 * conv2d.padding) - (conv2d.dilation * (conv2d.kernel_size - 1)) - 1) / conv2d.stride) + 1\n",
    "    height = math.floor(height)\n",
    "    \n",
    "    width = ((width + (2 * conv2d.padding) - (conv2d.dilation * (conv2d.kernel_size - 1)) - 1) / conv2d.stride) + 1\n",
    "    width = math.floor(width)\n",
    "\n",
    "    return (height, width)\n",
    "\n",
    "def convT_output_shape(height, width, convT: nn.Module):\n",
    "    height = (height - 1) * convT.stride[0] - (2 * convT.padding[0]) + (convT.dilation[0] * (convT.kernel_size[0] - 1)) + convT.output_padding[0] + 1\n",
    "    \n",
    "    width = (width - 1) * convT.stride[1] - (2 * convT.padding[1]) + (convT.dilation[1] * (convT.kernel_size[1] - 1)) + convT.output_padding[1] + 1\n",
    "    \n",
    "    return (height, width, convT.out_channels)\n",
    "\n",
    "\n",
    "def output_shapes(net : U_Net, image):\n",
    "    h = image.size(-2)  # height\n",
    "    w = image.size(-1)  # width\n",
    "    depth = len(net.encoders) + 1\n",
    "\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"CNN with depth = {depth}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    i = 1\n",
    "    for enc in net.encoders:\n",
    "        print(f\"||Layer {i} - encoders||\")\n",
    "\n",
    "        for l in enc.layers:\n",
    "            if not isinstance(l, nn.ReLU):\n",
    "                (h,w,c) = conv2d_output_shape(h, w, l)\n",
    "                print (f\"Conv2D -- H = {h} | W = {w} | C = {c}\")\n",
    "\n",
    "        (h,w) = pool_output_shape(h, w, enc.pooling)\n",
    "        print (f\"Pooling -- H = {h} | W = {w} | C = {c}\") \n",
    "\n",
    "        print()\n",
    "        i += 1\n",
    "            \n",
    "    print(f\"||Layer {i} - bridge||\")\n",
    "    (h,w,c) = conv2d_output_shape(h, w, net.bridge)\n",
    "    print (f\"H = {h} | W = {w} | C = {c}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    for dec in net.decoders:\n",
    "        print(f\"||Layer {i} - decoders||\")\n",
    "\n",
    "        (h,w,c) = convT_output_shape(h, w, dec.convT)\n",
    "        print (f\"Transpose -- H = {h} | W = {w} | C = {c}\")   \n",
    "\n",
    "        for l in dec.layers:\n",
    "            if not isinstance(l, nn.ReLU):\n",
    "                (h,w,c) = conv2d_output_shape(h, w, l)\n",
    "                print (f\"Conv2D -- H = {h} | W = {w} | C = {c}\")   \n",
    "\n",
    "        print()\n",
    "        i -= 1\n",
    "\n",
    "\n",
    "    print(f\"||Layer {i} - output||\")\n",
    "    (h,w,c) = conv2d_output_shape(h, w, net.output_layer)\n",
    "    print (f\"H = {h} | W = {w} | C = {c}\")\n",
    "    print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'images_array.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load data from .npy files\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages_array.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Assuming X.npy contains your images\u001b[39;00m\n\u001b[0;32m     10\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minst_maps_replaced_resized.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Assuming y.npy contains your labels\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Split the dataset into train, validation, and test sets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matul\\anaconda3\\envs\\NN\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'images_array.npy'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%run config.ipynb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load data from .npy files\n",
    "x = np.load('images_array.npy')  # Assuming X.npy contains your images\n",
    "y = np.load('inst_maps_replaced_resized.npy')  # Assuming y.npy contains your labels\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42) \n",
    "\n",
    "print(X_train)\n",
    "\n",
    "#net = U_Net(2, 10, config_to_dict(config_unet))\n",
    "\n",
    "#output_shapes(net, x)\n",
    "\n",
    "#out = net(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
