{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torchvision import models\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net Network\n",
    "num_classes = Number of classes expecteed from output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_layer(channels_in, channels_out, kernel_size, padding, stride, dilation):\n",
    "    layer = nn.ModuleList()\n",
    "  \n",
    "    layer.append(nn.Conv2d(\n",
    "        in_channels = channels_in, \n",
    "        out_channels = channels_out, \n",
    "        kernel_size = kernel_size, \n",
    "        stride = stride, \n",
    "        padding = padding, \n",
    "        dilation = dilation\n",
    "        )\n",
    "    )\n",
    "    layer.append(nn.ReLU())\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Block(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, params: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        # Config parameters\n",
    "        try:\n",
    "            block_width = params['block_width']\n",
    "            kernel_size = params['kernel_size']\n",
    "            padding = params['padding']\n",
    "            stride = params['stride']\n",
    "            dilation = params['dilation']\n",
    "            pool_type = params['pool_type']\n",
    "            pool_size = params['pool_kernel_size']\n",
    "        except KeyError as e:\n",
    "            raise Exception(f'Parameter \"{e.args[0]}\" NOT found!')\n",
    "        \n",
    "        # Add first Conv2D layer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.extend(conv2d_layer(channels_in, channels_out, kernel_size, padding, stride, dilation))\n",
    "\n",
    "        # Add other Conv2D layers\n",
    "        for _ in range(block_width - 1):\n",
    "            self.layers.extend(conv2d_layer(channels_out, channels_out, kernel_size, padding, stride, dilation))\n",
    "\n",
    "        # Add pooling layer\n",
    "        match pool_type:\n",
    "            case 'max':\n",
    "                self.pooling = nn.MaxPool2d(kernel_size=pool_size, stride=pool_size)\n",
    "            case 'avg':\n",
    "                self.pooling = nn.AvgPool2d(kernel_size=pool_size, stride=pool_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Pooling\n",
    "        return (self.pooling(x), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Block(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, padding_t, params: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        # Config parameters\n",
    "        try:\n",
    "            block_width = params['block_width']\n",
    "            kernel_size = params['kernel_size']\n",
    "            padding = params['padding']\n",
    "            stride = params['stride']\n",
    "            dilation = params['dilation']\n",
    "            pool_size = params['pool_kernel_size']\n",
    "        except KeyError as e:\n",
    "            raise Exception(f'Parameter \"{e.args[0]}\" NOT found!')\n",
    "\n",
    "        # Add transpose convolution\n",
    "        self.convT = nn.ConvTranspose2d (\n",
    "            in_channels = channels_in, \n",
    "            out_channels = math.floor(channels_in / 2), \n",
    "            kernel_size = pool_size, \n",
    "            stride = pool_size, \n",
    "            output_padding = padding_t\n",
    "        )\n",
    "\n",
    "        # Add first Conv2D layer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.extend(conv2d_layer(channels_in, channels_out, kernel_size, padding, stride, dilation))\n",
    "\n",
    "        # Add other Conv2D layers\n",
    "        for _ in range(block_width - 1):\n",
    "            self.layers.extend(conv2d_layer(channels_out, channels_out, kernel_size, padding, stride, dilation))\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_features):\n",
    "        # Transposed convolution\n",
    "        x = self.convT(x)\n",
    "\n",
    "        # Copy and crop features\n",
    "        w = x.size(-1)\n",
    "        c = math.floor((enc_features.size(-1) - w) / 2)\n",
    "\n",
    "        print(x.size())\n",
    "        print(x.size(dim=1))\n",
    "        \n",
    "        print(enc_features.size())\n",
    "        print(enc_features.size(dim=1))\n",
    "        #enc_features = enc_features[:, :, c:c+w , c:c+w]\n",
    "        x = torch.cat([enc_features, x], dim = 1)\n",
    "\n",
    "        # Convolution\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_Net(nn.Module):\n",
    "    def __init__(self, channels_in, num_classes, params: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        # Config parameters\n",
    "        try:\n",
    "            channel_mul = params['channel_mul']\n",
    "            depth = params['network_depth']\n",
    "            channels_out = params['channels_out_init']\n",
    "            padding_convT = params['padding_convT']\n",
    "        except KeyError as e:\n",
    "            raise Exception(f'Parameter \"{e.args[0]}\" NOT found!')\n",
    "        \n",
    "        # Create encoder\n",
    "        self.encoders = nn.ModuleList()\n",
    "\n",
    "        for _ in range(depth) :\n",
    "            self.encoders.append(Encoder_Block(channels_in, channels_out, params))\n",
    "            channels_in = channels_out\n",
    "            channels_out = math.floor(channels_out * channel_mul)\n",
    "\n",
    "        # Create bridge as Conv2D layer\n",
    "        last_encoder = Encoder_Block(channels_in, channels_out, params)\n",
    "        self.bridge = last_encoder.layers[0]\n",
    "\n",
    "        channels_in = channels_out\n",
    "        channels_out = math.floor(channels_in / channel_mul)\n",
    "\n",
    "        # Create decoder\n",
    "        self.decoders = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(depth) :\n",
    "            paddingT = padding_convT.pop(0)\n",
    "            self.decoders.append(Decoder_Block(channels_in, channels_out, paddingT, params))\n",
    "            channels_in = channels_out\n",
    "            channels_out = math.floor(channels_out / channel_mul)\n",
    "\n",
    "        # Create output layer (1x1 convolution, return logits)\n",
    "        self.output_layer = nn.Conv2d(\n",
    "            in_channels = channels_in, \n",
    "            out_channels = num_classes, \n",
    "            kernel_size = 1\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Features created during enconding\n",
    "        features = []\n",
    "\n",
    "        # Encoding\n",
    "        for enc in self.encoders:\n",
    "            x, conv_output = enc(x)\n",
    "            features.append(conv_output)\n",
    "\n",
    "        # Bridge between encoder and decoder\n",
    "        x = self.bridge(x)\n",
    "\n",
    "        # Decoding\n",
    "        for dec in self.decoders:\n",
    "            enc_output = features.pop()\n",
    "            x = dec(x, enc_output)\n",
    "\n",
    "        # Return logits\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_output_shape(height, width, conv2d: nn.Module):\n",
    "    height = ((height + (2 * conv2d.padding[0]) - (conv2d.dilation[0] * (conv2d.kernel_size[0] - 1)) - 1) / conv2d.stride[0]) + 1\n",
    "    height = math.floor(height)\n",
    "    \n",
    "    width = ((width + (2 * conv2d.padding[1]) - (conv2d.dilation[1] * (conv2d.kernel_size[1] - 1)) - 1) / conv2d.stride[1]) + 1\n",
    "    width = math.floor(width)\n",
    "\n",
    "    return (height, width, conv2d.out_channels)\n",
    "\n",
    "def pool_output_shape(height, width, conv2d: nn.Module):\n",
    "    height = ((height + (2 * conv2d.padding) - (conv2d.dilation * (conv2d.kernel_size - 1)) - 1) / conv2d.stride) + 1\n",
    "    height = math.floor(height)\n",
    "    \n",
    "    width = ((width + (2 * conv2d.padding) - (conv2d.dilation * (conv2d.kernel_size - 1)) - 1) / conv2d.stride) + 1\n",
    "    width = math.floor(width)\n",
    "\n",
    "    return (height, width)\n",
    "\n",
    "def convT_output_shape(height, width, convT: nn.Module):\n",
    "    height = (height - 1) * convT.stride[0] - (2 * convT.padding[0]) + (convT.dilation[0] * (convT.kernel_size[0] - 1)) + convT.output_padding[0] + 1\n",
    "    \n",
    "    width = (width - 1) * convT.stride[1] - (2 * convT.padding[1]) + (convT.dilation[1] * (convT.kernel_size[1] - 1)) + convT.output_padding[1] + 1\n",
    "    \n",
    "    return (height, width, convT.out_channels)\n",
    "\n",
    "\n",
    "def output_shapes(net : U_Net, image):\n",
    "    h = image.size(-2)  # height\n",
    "    w = image.size(-1)  # width\n",
    "    depth = len(net.encoders) + 1\n",
    "\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"CNN with depth = {depth}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    i = 1\n",
    "    for enc in net.encoders:\n",
    "        print(f\"||Layer {i} - encoders||\")\n",
    "\n",
    "        for l in enc.layers:\n",
    "            if not isinstance(l, nn.ReLU):\n",
    "                (h,w,c) = conv2d_output_shape(h, w, l)\n",
    "                print (f\"Conv2D -- H = {h} | W = {w} | C = {c}\")\n",
    "\n",
    "        (h,w) = pool_output_shape(h, w, enc.pooling)\n",
    "        print (f\"Pooling -- H = {h} | W = {w} | C = {c}\") \n",
    "\n",
    "        print()\n",
    "        i += 1\n",
    "            \n",
    "    print(f\"||Layer {i} - bridge||\")\n",
    "    (h,w,c) = conv2d_output_shape(h, w, net.bridge)\n",
    "    print (f\"H = {h} | W = {w} | C = {c}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    for dec in net.decoders:\n",
    "        print(f\"||Layer {i} - decoders||\")\n",
    "\n",
    "        (h,w,c) = convT_output_shape(h, w, dec.convT)\n",
    "        print (f\"Transpose -- H = {h} | W = {w} | C = {c}\")   \n",
    "\n",
    "        for l in dec.layers:\n",
    "            if not isinstance(l, nn.ReLU):\n",
    "                (h,w,c) = conv2d_output_shape(h, w, l)\n",
    "                print (f\"Conv2D -- H = {h} | W = {w} | C = {c}\")   \n",
    "\n",
    "        print()\n",
    "        i -= 1\n",
    "\n",
    "\n",
    "    print(f\"||Layer {i} - output||\")\n",
    "    (h,w,c) = conv2d_output_shape(h, w, net.output_layer)\n",
    "    print (f\"H = {h} | W = {w} | C = {c}\")\n",
    "    print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[111  27  98]\n",
      "   [114  29 108]\n",
      "   [114  23 119]\n",
      "   ...\n",
      "   [213  98 161]\n",
      "   [248 142 182]\n",
      "   [251 170 199]]\n",
      "\n",
      "  [[123  31 110]\n",
      "   [127  34 118]\n",
      "   [127  30 130]\n",
      "   ...\n",
      "   [239 147 176]\n",
      "   [253 182 201]\n",
      "   [254 197 241]]\n",
      "\n",
      "  [[140  34 135]\n",
      "   [138  32 125]\n",
      "   [140  35 123]\n",
      "   ...\n",
      "   [251 173 189]\n",
      "   [253 202 225]\n",
      "   [253 210 247]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[237 131 195]\n",
      "   [247 151 199]\n",
      "   [225 135 206]\n",
      "   ...\n",
      "   [189 104 207]\n",
      "   [169  84 170]\n",
      "   [170  76 176]]\n",
      "\n",
      "  [[232 134 197]\n",
      "   [251 165 209]\n",
      "   [243 164 215]\n",
      "   ...\n",
      "   [179  91 203]\n",
      "   [167  80 166]\n",
      "   [170  78 157]]\n",
      "\n",
      "  [[216 120 203]\n",
      "   [238 154 213]\n",
      "   [250 177 216]\n",
      "   ...\n",
      "   [171  84 197]\n",
      "   [154  66 155]\n",
      "   [177  83 166]]]\n",
      "\n",
      "\n",
      " [[[233 221 231]\n",
      "   [233 221 231]\n",
      "   [233 221 231]\n",
      "   ...\n",
      "   [238 226 236]\n",
      "   [238 226 235]\n",
      "   [237 226 234]]\n",
      "\n",
      "  [[233 221 231]\n",
      "   [233 221 231]\n",
      "   [233 221 231]\n",
      "   ...\n",
      "   [237 225 235]\n",
      "   [237 226 234]\n",
      "   [237 226 234]]\n",
      "\n",
      "  [[233 221 231]\n",
      "   [233 221 231]\n",
      "   [233 221 231]\n",
      "   ...\n",
      "   [240 229 238]\n",
      "   [237 225 235]\n",
      "   [235 224 232]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[174 117 173]\n",
      "   [190 131 192]\n",
      "   [211 152 217]\n",
      "   ...\n",
      "   [189 167 201]\n",
      "   [192 169 207]\n",
      "   [188 167 202]]\n",
      "\n",
      "  [[204 150 198]\n",
      "   [164 105 163]\n",
      "   [173 108 180]\n",
      "   ...\n",
      "   [183 156 189]\n",
      "   [159 132 166]\n",
      "   [188 163 195]]\n",
      "\n",
      "  [[225 170 217]\n",
      "   [197 136 196]\n",
      "   [156  86 165]\n",
      "   ...\n",
      "   [185 156 188]\n",
      "   [167 138 168]\n",
      "   [204 179 206]]]\n",
      "\n",
      "\n",
      " [[[242 241 243]\n",
      "   [243 241 243]\n",
      "   [242 241 243]\n",
      "   ...\n",
      "   [233 212 235]\n",
      "   [223 198 224]\n",
      "   [237 216 238]]\n",
      "\n",
      "  [[243 241 243]\n",
      "   [243 241 244]\n",
      "   [242 241 243]\n",
      "   ...\n",
      "   [233 213 233]\n",
      "   [232 212 229]\n",
      "   [230 213 230]]\n",
      "\n",
      "  [[243 241 243]\n",
      "   [242 241 243]\n",
      "   [242 241 243]\n",
      "   ...\n",
      "   [223 202 224]\n",
      "   [218 195 219]\n",
      "   [214 192 215]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[246 230 240]\n",
      "   [200 161 203]\n",
      "   [184 145 200]\n",
      "   ...\n",
      "   [242 228 239]\n",
      "   [201 177 205]\n",
      "   [234 214 236]]\n",
      "\n",
      "  [[247 230 239]\n",
      "   [209 171 209]\n",
      "   [163 128 190]\n",
      "   ...\n",
      "   [250 244 246]\n",
      "   [242 233 244]\n",
      "   [212 200 217]]\n",
      "\n",
      "  [[234 214 230]\n",
      "   [222 187 218]\n",
      "   [193 161 208]\n",
      "   ...\n",
      "   [242 240 242]\n",
      "   [248 244 247]\n",
      "   [233 227 235]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[215 176 216]\n",
      "   [216 174 214]\n",
      "   [211 169 213]\n",
      "   ...\n",
      "   [198 156 212]\n",
      "   [180 138 195]\n",
      "   [188 146 202]]\n",
      "\n",
      "  [[210 169 213]\n",
      "   [213 173 216]\n",
      "   [201 162 206]\n",
      "   ...\n",
      "   [196 151 204]\n",
      "   [192 149 199]\n",
      "   [203 162 205]]\n",
      "\n",
      "  [[204 165 210]\n",
      "   [203 169 214]\n",
      "   [187 155 206]\n",
      "   ...\n",
      "   [209 170 214]\n",
      "   [209 170 210]\n",
      "   [216 176 216]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[200 158 204]\n",
      "   [212 171 213]\n",
      "   [220 178 218]\n",
      "   ...\n",
      "   [203 158 210]\n",
      "   [189 147 202]\n",
      "   [175 136 196]]\n",
      "\n",
      "  [[212 171 215]\n",
      "   [216 175 214]\n",
      "   [215 174 212]\n",
      "   ...\n",
      "   [211 164 212]\n",
      "   [199 153 206]\n",
      "   [191 146 202]]\n",
      "\n",
      "  [[201 159 209]\n",
      "   [194 151 197]\n",
      "   [213 172 211]\n",
      "   ...\n",
      "   [201 154 206]\n",
      "   [202 153 206]\n",
      "   [186 138 195]]]\n",
      "\n",
      "\n",
      " [[[244 241 244]\n",
      "   [244 241 244]\n",
      "   [244 241 243]\n",
      "   ...\n",
      "   [207 157 211]\n",
      "   [203 153 208]\n",
      "   [195 148 203]]\n",
      "\n",
      "  [[244 241 244]\n",
      "   [245 242 244]\n",
      "   [243 241 243]\n",
      "   ...\n",
      "   [215 166 217]\n",
      "   [223 176 226]\n",
      "   [227 182 232]]\n",
      "\n",
      "  [[244 241 244]\n",
      "   [246 243 245]\n",
      "   [244 242 244]\n",
      "   ...\n",
      "   [229 182 231]\n",
      "   [222 176 225]\n",
      "   [237 192 243]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 90  46 130]\n",
      "   [ 89  49 122]\n",
      "   [181 139 192]\n",
      "   ...\n",
      "   [243 204 239]\n",
      "   [254 214 247]\n",
      "   [251 208 239]]\n",
      "\n",
      "  [[100  56 149]\n",
      "   [ 85  42 126]\n",
      "   [177 140 193]\n",
      "   ...\n",
      "   [252 222 244]\n",
      "   [255 227 250]\n",
      "   [253 223 245]]\n",
      "\n",
      "  [[ 89  44 141]\n",
      "   [107  61 154]\n",
      "   [185 142 209]\n",
      "   ...\n",
      "   [250 213 233]\n",
      "   [255 218 238]\n",
      "   [254 214 239]]]\n",
      "\n",
      "\n",
      " [[[241 234 242]\n",
      "   [241 234 242]\n",
      "   [240 233 240]\n",
      "   ...\n",
      "   [185  66 147]\n",
      "   [173  53 132]\n",
      "   [169  52 128]]\n",
      "\n",
      "  [[241 234 241]\n",
      "   [241 234 241]\n",
      "   [240 233 240]\n",
      "   ...\n",
      "   [208  77 161]\n",
      "   [206  74 157]\n",
      "   [172  51 129]]\n",
      "\n",
      "  [[241 234 240]\n",
      "   [241 235 239]\n",
      "   [241 234 241]\n",
      "   ...\n",
      "   [202  61 143]\n",
      "   [203  67 147]\n",
      "   [179  53 130]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[232 154 210]\n",
      "   [234 160 214]\n",
      "   [223 153 206]\n",
      "   ...\n",
      "   [158  84 168]\n",
      "   [162  84 170]\n",
      "   [170  87 174]]\n",
      "\n",
      "  [[216 148 198]\n",
      "   [212 145 196]\n",
      "   [213 147 201]\n",
      "   ...\n",
      "   [178  94 178]\n",
      "   [186 100 185]\n",
      "   [179  93 176]]\n",
      "\n",
      "  [[219 154 209]\n",
      "   [206 143 197]\n",
      "   [214 151 205]\n",
      "   ...\n",
      "   [184  97 179]\n",
      "   [186  96 181]\n",
      "   [181  93 176]]]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%run config.ipynb\n",
    "\n",
    "# Load data from .npy files\n",
    "x = np.load('images_array.npy')  # Assuming X.npy contains your images\n",
    "y = np.load('inst_maps_replaced_resized.npy')  # Assuming y.npy contains your labels\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42) \n",
    "\n",
    "print(X_train)\n",
    "\n",
    "#net = U_Net(2, 10, config_to_dict(config_unet))\n",
    "\n",
    "#output_shapes(net, x)\n",
    "\n",
    "#out = net(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
