{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torchvision import models\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net Network\n",
    "num_classes = Number of classes expecteed from output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_layer(channels_in, channels_out, kernel_size, padding, stride, dilation):\n",
    "    layer = nn.ModuleList()\n",
    "  \n",
    "    layer.append(nn.Conv2d(\n",
    "        in_channels = channels_in, \n",
    "        out_channels = channels_out, \n",
    "        kernel_size = kernel_size, \n",
    "        stride = stride, \n",
    "        padding = padding, \n",
    "        dilation = dilation\n",
    "        )\n",
    "    )\n",
    "    layer.append(nn.ReLU())\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Block(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, params: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        # Config parameters\n",
    "        try:\n",
    "            block_width = params['block_width']\n",
    "            kernel_size = params['kernel_size']\n",
    "            padding = params['padding']\n",
    "            stride = params['stride']\n",
    "            dilation = params['dilation']\n",
    "            pool_type = params['pool_type']\n",
    "            pool_size = params['pool_kernel_size']\n",
    "        except KeyError as e:\n",
    "            raise Exception(f'Parameter \"{e.args[0]}\" NOT found!')\n",
    "        \n",
    "        # Add first Conv2D layer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.extend(conv2d_layer(channels_in, channels_out, kernel_size, padding, stride, dilation))\n",
    "\n",
    "        # Add other Conv2D layers\n",
    "        for _ in range(block_width - 1):\n",
    "            self.layers.extend(conv2d_layer(channels_out, channels_out, kernel_size, padding, stride, dilation))\n",
    "\n",
    "        # Add pooling layer\n",
    "        match pool_type:\n",
    "            case 'max':\n",
    "                self.pooling = nn.MaxPool2d(kernel_size=pool_size, stride=pool_size)\n",
    "            case 'avg':\n",
    "                self.pooling = nn.AvgPool2d(kernel_size=pool_size, stride=pool_size)\n",
    "\n",
    "\n",
    "    def forward_noSkip(self, x):\n",
    "        # Convolution\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Pooling\n",
    "        pooled = self.pooling(x)\n",
    "\n",
    "        return pooled\n",
    "    \n",
    "    def forward_skip(self, x):\n",
    "        # Convolution\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Pooling\n",
    "        pooled = self.pooling(x)\n",
    "\n",
    "        return (pooled, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Block(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, padding_t, params: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        # Config parameters\n",
    "        try:\n",
    "            block_width = params['block_width']\n",
    "            kernel_size = params['kernel_size']\n",
    "            padding = params['padding']\n",
    "            stride = params['stride']\n",
    "            dilation = params['dilation']\n",
    "            pool_size = params['pool_kernel_size']\n",
    "            skip_features = params['skip_features']\n",
    "        except KeyError as e:\n",
    "            raise Exception(f'Parameter \"{e.args[0]}\" NOT found!')\n",
    "\n",
    "        # Add transpose convolution\n",
    "        self.convT = nn.ConvTranspose2d (\n",
    "            in_channels = channels_in, \n",
    "            out_channels = math.floor(channels_in / 2), \n",
    "            kernel_size = pool_size, \n",
    "            stride = pool_size, \n",
    "            output_padding = padding_t\n",
    "        )\n",
    "\n",
    "        match skip_features:\n",
    "            case \"concat\":\n",
    "                channels_in = math.floor(channels_in / 2)\n",
    "                pass\n",
    "            case \"none\":\n",
    "                pass\n",
    "            case \"sdi\":\n",
    "                pass\n",
    "\n",
    "\n",
    "        # Add first Conv2D layer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.extend(conv2d_layer(channels_in, channels_out, kernel_size, padding, stride, dilation))\n",
    "\n",
    "        # Add other Conv2D layers\n",
    "        for _ in range(block_width - 1):\n",
    "            self.layers.extend(conv2d_layer(channels_out, channels_out, kernel_size, padding, stride, dilation))\n",
    "\n",
    "    def forward_noSkip(self, x):\n",
    "        # Transposed convolution\n",
    "        x = self.convT(x)\n",
    "\n",
    "        # Convolution\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_skip(self, x, enc_feature):\n",
    "        # Transposed convolution\n",
    "        x = self.convT(x)\n",
    "\n",
    "        # Copy and crop features\n",
    "        w = x.size(-1)\n",
    "        c = math.floor((enc_feature.size(-1) - w) / 2)\n",
    "\n",
    "        print(x.size())   \n",
    "        print(enc_feature.size())\n",
    "        #enc_features = enc_features[:, :, c:c+w , c:c+w]\n",
    "        x = torch.cat([enc_feature, x], dim = 1)\n",
    "\n",
    "        # Convolution\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_Net(nn.Module):\n",
    "    def __init__(self, channels_in, num_classes, params: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        # Config parameters\n",
    "        try:\n",
    "            channel_mul = params['channel_mul']\n",
    "            depth = params['network_depth']\n",
    "            channels_out = params['channels_out_init']\n",
    "            padding_convT = params['padding_convT']\n",
    "            self.skip_features = params['skip_features']\n",
    "        except KeyError as e:\n",
    "            raise Exception(f'Parameter \"{e.args[0]}\" NOT found!')\n",
    "        \n",
    "        # Create encoder\n",
    "        self.encoders = nn.ModuleList()\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.encoders.append(Encoder_Block(channels_in, channels_out, params))\n",
    "            channels_in = channels_out\n",
    "            channels_out = math.floor(channels_out * channel_mul)\n",
    "\n",
    "        # Create bridge as Conv2D layer\n",
    "        last_encoder = Encoder_Block(channels_in, channels_out, params)\n",
    "        self.bridge = last_encoder.layers[0]\n",
    "\n",
    "        channels_in = channels_out\n",
    "        channels_out = math.floor(channels_in / channel_mul)\n",
    "\n",
    "        # Create decoder\n",
    "        self.decoders = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(depth) :\n",
    "            paddingT = padding_convT.pop(0)\n",
    "            self.decoders.append(Decoder_Block(channels_in, channels_out, paddingT, params))\n",
    "            channels_in = channels_out\n",
    "            channels_out = math.floor(channels_out / channel_mul)\n",
    "\n",
    "        # Create output layer (1x1 convolution, return logits)\n",
    "        self.output_layer = nn.Conv2d(\n",
    "            in_channels = channels_in, \n",
    "            out_channels = num_classes, \n",
    "            kernel_size = 1\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        match self.skip_features:\n",
    "            # No skipping\n",
    "            case \"none\":\n",
    "                print(\"This U-Net does NOT skip any connections!\")\n",
    "                # Encoding\n",
    "                for enc in self.encoders:\n",
    "                    x = enc.forward_noSkip(x)\n",
    "\n",
    "                # Bridge between encoder and decoder\n",
    "                x = self.bridge(x)\n",
    "\n",
    "                # Decoding\n",
    "                for dec in self.decoders:\n",
    "                    x = dec.forward_noSkip(x)\n",
    "\n",
    "            # Cat tensors => features + pooled output\n",
    "            case \"concat\":\n",
    "                print(\"This U-Net skips connections using concatenation!\")\n",
    "                # Features created during enconding\n",
    "                features = []\n",
    "\n",
    "                # Encoding\n",
    "                for enc in self.encoders:\n",
    "                    x, enc_feature = enc.forward_skip(x)\n",
    "                    features.append(enc_feature)\n",
    "\n",
    "                # Bridge between encoder and decoder\n",
    "                x = self.bridge(x)\n",
    "\n",
    "                # Decoding\n",
    "                for dec in self.decoders:\n",
    "                    enc_output = features.pop()\n",
    "                    x = dec.forward_skip(x, enc_output)\n",
    "\n",
    "            case \"sdi\":\n",
    "                pass\n",
    "    \n",
    "\n",
    "        # Output as logits\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_output_shape(height, width, conv2d: nn.Module):\n",
    "    height = ((height + (2 * conv2d.padding[0]) - (conv2d.dilation[0] * (conv2d.kernel_size[0] - 1)) - 1) / conv2d.stride[0]) + 1\n",
    "    height = math.floor(height)\n",
    "    \n",
    "    width = ((width + (2 * conv2d.padding[1]) - (conv2d.dilation[1] * (conv2d.kernel_size[1] - 1)) - 1) / conv2d.stride[1]) + 1\n",
    "    width = math.floor(width)\n",
    "\n",
    "    return (height, width, conv2d.out_channels)\n",
    "\n",
    "def pool_output_shape(height, width, conv2d: nn.Module):\n",
    "    height = ((height + (2 * conv2d.padding) - (conv2d.dilation * (conv2d.kernel_size - 1)) - 1) / conv2d.stride) + 1\n",
    "    height = math.floor(height)\n",
    "    \n",
    "    width = ((width + (2 * conv2d.padding) - (conv2d.dilation * (conv2d.kernel_size - 1)) - 1) / conv2d.stride) + 1\n",
    "    width = math.floor(width)\n",
    "\n",
    "    return (height, width)\n",
    "\n",
    "def convT_output_shape(height, width, convT: nn.Module):\n",
    "    height = (height - 1) * convT.stride[0] - (2 * convT.padding[0]) + (convT.dilation[0] * (convT.kernel_size[0] - 1)) + convT.output_padding[0] + 1\n",
    "    \n",
    "    width = (width - 1) * convT.stride[1] - (2 * convT.padding[1]) + (convT.dilation[1] * (convT.kernel_size[1] - 1)) + convT.output_padding[1] + 1\n",
    "    \n",
    "    return (height, width, convT.out_channels)\n",
    "\n",
    "\n",
    "def output_shapes(net : U_Net, image):\n",
    "    h = image.size(-2)  # height\n",
    "    w = image.size(-1)  # width\n",
    "    depth = len(net.encoders) + 1\n",
    "\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"CNN with depth = {depth}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    i = 1\n",
    "    for enc in net.encoders:\n",
    "        print(f\"||Layer {i} - encoders||\")\n",
    "\n",
    "        for l in enc.layers:\n",
    "            if not isinstance(l, nn.ReLU):\n",
    "                (h,w,c) = conv2d_output_shape(h, w, l)\n",
    "                print (f\"Conv2D -- H = {h} | W = {w} | C = {c}\")\n",
    "\n",
    "        (h,w) = pool_output_shape(h, w, enc.pooling)\n",
    "        print (f\"Pooling -- H = {h} | W = {w} | C = {c}\") \n",
    "\n",
    "        print()\n",
    "        i += 1\n",
    "            \n",
    "    print(f\"||Layer {i} - bridge||\")\n",
    "    (h,w,c) = conv2d_output_shape(h, w, net.bridge)\n",
    "    print (f\"H = {h} | W = {w} | C = {c}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "    for dec in net.decoders:\n",
    "        print(f\"||Layer {i} - decoders||\")\n",
    "\n",
    "        (h,w,c) = convT_output_shape(h, w, dec.convT)\n",
    "        print (f\"Transpose -- H = {h} | W = {w} | C = {c}\")   \n",
    "\n",
    "        for l in dec.layers:\n",
    "            if not isinstance(l, nn.ReLU):\n",
    "                (h,w,c) = conv2d_output_shape(h, w, l)\n",
    "                print (f\"Conv2D -- H = {h} | W = {w} | C = {c}\")   \n",
    "\n",
    "        print()\n",
    "        i -= 1\n",
    "\n",
    "\n",
    "    print(f\"||Layer {i} - output||\")\n",
    "    (h,w,c) = conv2d_output_shape(h, w, net.output_layer)\n",
    "    print (f\"H = {h} | W = {w} | C = {c}\")\n",
    "    print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([238, 3, 500, 500])\n",
      "torch.Size([238, 3, 500, 500])\n",
      "torch.float64\n",
      "torch.uint8\n",
      "tensor([[[[208, 219, 175,  ..., 201, 221, 231],\n",
      "          [214, 226, 167,  ..., 195, 215, 213],\n",
      "          [186, 197, 154,  ..., 186, 192, 198],\n",
      "          ...,\n",
      "          [200, 192, 204,  ..., 207, 221, 234],\n",
      "          [199, 202, 210,  ..., 234, 246, 245],\n",
      "          [209, 209, 212,  ..., 252, 245, 223]],\n",
      "\n",
      "         [[183, 195, 149,  ..., 145, 170, 180],\n",
      "          [189, 202, 141,  ..., 144, 166, 165],\n",
      "          [160, 172, 128,  ..., 141, 149, 157],\n",
      "          ...,\n",
      "          [155, 146, 152,  ..., 185, 201, 215],\n",
      "          [152, 154, 159,  ..., 216, 229, 228],\n",
      "          [168, 169, 172,  ..., 238, 230, 207]],\n",
      "\n",
      "         [[234, 239, 207,  ..., 198, 222, 234],\n",
      "          [244, 246, 201,  ..., 199, 222, 223],\n",
      "          [226, 231, 194,  ..., 205, 211, 216],\n",
      "          ...,\n",
      "          [219, 209, 215,  ..., 222, 233, 244],\n",
      "          [212, 214, 219,  ..., 243, 252, 252],\n",
      "          [217, 216, 219,  ..., 254, 252, 236]]],\n",
      "\n",
      "\n",
      "        [[[202, 203, 197,  ..., 237, 240, 232],\n",
      "          [194, 197, 191,  ..., 235, 241, 234],\n",
      "          [199, 192, 208,  ..., 218, 225, 235],\n",
      "          ...,\n",
      "          [159, 163, 197,  ..., 201, 203, 215],\n",
      "          [146, 142, 152,  ..., 199, 199, 227],\n",
      "          [167, 155, 139,  ..., 213, 205, 213]],\n",
      "\n",
      "         [[165, 167, 159,  ..., 228, 231, 224],\n",
      "          [159, 161, 155,  ..., 226, 232, 224],\n",
      "          [163, 156, 171,  ..., 209, 216, 226],\n",
      "          ...,\n",
      "          [125, 132, 156,  ..., 162, 165, 178],\n",
      "          [112, 108, 112,  ..., 160, 161, 191],\n",
      "          [131, 117,  99,  ..., 175, 168, 178]],\n",
      "\n",
      "         [[231, 234, 227,  ..., 242, 244, 237],\n",
      "          [207, 217, 214,  ..., 240, 246, 238],\n",
      "          [217, 214, 232,  ..., 224, 231, 241],\n",
      "          ...,\n",
      "          [200, 202, 230,  ..., 215, 216, 225],\n",
      "          [182, 178, 183,  ..., 214, 216, 244],\n",
      "          [199, 189, 169,  ..., 230, 224, 235]]],\n",
      "\n",
      "\n",
      "        [[[148, 156, 132,  ..., 210, 210, 237],\n",
      "          [141, 152, 139,  ..., 229, 244, 233],\n",
      "          [153, 138, 109,  ..., 222, 215, 197],\n",
      "          ...,\n",
      "          [204, 206, 231,  ..., 236, 229, 216],\n",
      "          [221, 217, 246,  ..., 241, 209, 211],\n",
      "          [221, 219, 232,  ..., 251, 215, 198]],\n",
      "\n",
      "         [[125, 133, 109,  ..., 163, 162, 190],\n",
      "          [116, 130, 119,  ..., 183, 196, 185],\n",
      "          [126, 116,  90,  ..., 177, 168, 148],\n",
      "          ...,\n",
      "          [179, 185, 213,  ..., 190, 184, 173],\n",
      "          [196, 195, 229,  ..., 195, 163, 167],\n",
      "          [195, 198, 213,  ..., 210, 169, 155]],\n",
      "\n",
      "         [[194, 211, 195,  ..., 220, 220, 245],\n",
      "          [188, 207, 200,  ..., 233, 250, 247],\n",
      "          [202, 193, 168,  ..., 225, 219, 212],\n",
      "          ...,\n",
      "          [212, 213, 238,  ..., 243, 237, 224],\n",
      "          [230, 224, 253,  ..., 248, 220, 220],\n",
      "          [232, 227, 238,  ..., 254, 231, 209]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[196, 196, 205,  ..., 218, 199, 150],\n",
      "          [196, 198, 206,  ..., 187, 212, 205],\n",
      "          [195, 203, 212,  ..., 152, 217, 255],\n",
      "          ...,\n",
      "          [104, 107, 107,  ...,  84,  65,  70],\n",
      "          [105, 106, 107,  ...,  84,  65,  72],\n",
      "          [106, 105, 107,  ...,  98,  73,  79]],\n",
      "\n",
      "         [[137, 140, 150,  ..., 206, 183, 124],\n",
      "          [138, 142, 152,  ..., 170, 195, 192],\n",
      "          [139, 147, 160,  ..., 128, 196, 255],\n",
      "          ...,\n",
      "          [ 61,  64,  63,  ...,  66,  55,  55],\n",
      "          [ 62,  63,  62,  ...,  63,  49,  53],\n",
      "          [ 63,  61,  62,  ...,  76,  54,  57]],\n",
      "\n",
      "         [[209, 209, 217,  ..., 255, 244, 213],\n",
      "          [210, 211, 218,  ..., 220, 237, 235],\n",
      "          [209, 216, 224,  ..., 178, 225, 255],\n",
      "          ...,\n",
      "          [140, 147, 150,  ..., 141, 131, 126],\n",
      "          [142, 147, 152,  ..., 154, 142, 130],\n",
      "          [145, 147, 153,  ..., 171, 152, 134]]],\n",
      "\n",
      "\n",
      "        [[[203, 218, 213,  ..., 203, 200, 225],\n",
      "          [196, 221, 209,  ..., 219, 197, 207],\n",
      "          [214, 216, 170,  ..., 208, 206, 210],\n",
      "          ...,\n",
      "          [188, 180, 160,  ..., 230, 204, 185],\n",
      "          [163, 177, 169,  ..., 235, 194, 173],\n",
      "          [160, 179, 172,  ..., 208, 194, 178]],\n",
      "\n",
      "         [[143, 159, 156,  ..., 152, 150, 175],\n",
      "          [136, 162, 152,  ..., 168, 145, 156],\n",
      "          [156, 158, 113,  ..., 155, 154, 156],\n",
      "          ...,\n",
      "          [138, 132, 113,  ..., 195, 164, 140],\n",
      "          [113, 128, 121,  ..., 198, 155, 129],\n",
      "          [110, 129, 122,  ..., 170, 154, 134]],\n",
      "\n",
      "         [[200, 214, 208,  ..., 203, 200, 225],\n",
      "          [196, 221, 208,  ..., 218, 195, 206],\n",
      "          [217, 221, 175,  ..., 206, 203, 206],\n",
      "          ...,\n",
      "          [212, 201, 176,  ..., 241, 216, 195],\n",
      "          [186, 201, 194,  ..., 246, 205, 183],\n",
      "          [185, 203, 198,  ..., 220, 204, 185]]],\n",
      "\n",
      "\n",
      "        [[[232, 219, 219,  ..., 242, 242, 242],\n",
      "          [224, 215, 215,  ..., 242, 242, 243],\n",
      "          [215, 219, 223,  ..., 242, 243, 243],\n",
      "          ...,\n",
      "          [244, 243, 243,  ..., 242, 242, 242],\n",
      "          [244, 243, 243,  ..., 241, 241, 242],\n",
      "          [243, 243, 243,  ..., 240, 241, 241]],\n",
      "\n",
      "         [[156, 142, 142,  ..., 238, 238, 238],\n",
      "          [148, 137, 138,  ..., 239, 238, 238],\n",
      "          [136, 140, 144,  ..., 239, 239, 238],\n",
      "          ...,\n",
      "          [241, 240, 240,  ..., 242, 242, 242],\n",
      "          [240, 240, 240,  ..., 242, 242, 242],\n",
      "          [240, 240, 240,  ..., 241, 242, 242]],\n",
      "\n",
      "         [[204, 191, 192,  ..., 243, 243, 242],\n",
      "          [196, 185, 185,  ..., 243, 243, 242],\n",
      "          [184, 188, 192,  ..., 243, 243, 243],\n",
      "          ...,\n",
      "          [244, 244, 244,  ..., 242, 242, 242],\n",
      "          [244, 243, 243,  ..., 242, 241, 241],\n",
      "          [243, 243, 243,  ..., 241, 241, 241]]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%run config.ipynb\n",
    "\n",
    "x = torch.load('tensor_images.pt')\n",
    "y = torch.load('tensor_labels.pt')\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(x.dtype)\n",
    "print(y.dtype)\n",
    "\n",
    "\n",
    "print(y)\n",
    "\n",
    "\n",
    "\n",
    "#net = U_Net(2, 10, config_to_dict(config_unet))\n",
    "\n",
    "#output_shapes(net, x)\n",
    "\n",
    "#out = net(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
